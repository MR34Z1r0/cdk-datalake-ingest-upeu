{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbc2822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in c:\\programdata\\anaconda3\\envs\\env_datalake\\lib\\site-packages (1.40.0)\n",
      "Requirement already satisfied: pytz in c:\\programdata\\anaconda3\\envs\\env_datalake\\lib\\site-packages (2025.2)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\envs\\env_datalake\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: aws_lambda_powertools in c:\\programdata\\anaconda3\\envs\\env_datalake\\lib\\site-packages (3.18.0)\n",
      "Requirement already satisfied: pyodbc in c:\\programdata\\anaconda3\\envs\\env_datalake\\lib\\site-packages (5.2.0)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-21.0.0-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.0 in c:\\programdata\\anaconda3\\envs\\env_datalake\\lib\\site-packages (from boto3) (1.40.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\programdata\\anaconda3\\envs\\env_datalake\\lib\\site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in c:\\programdata\\anaconda3\\envs\\env_datalake\\lib\\site-packages (from boto3) (0.13.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\programdata\\anaconda3\\envs\\env_datalake\\lib\\site-packages (from botocore<1.41.0,>=1.40.0->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\programdata\\anaconda3\\envs\\env_datalake\\lib\\site-packages (from botocore<1.41.0,>=1.40.0->boto3) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\envs\\env_datalake\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.0->boto3) (1.17.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\programdata\\anaconda3\\envs\\env_datalake\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\envs\\env_datalake\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\programdata\\anaconda3\\envs\\env_datalake\\lib\\site-packages (from aws_lambda_powertools) (4.14.1)\n",
      "Downloading pyarrow-21.0.0-cp310-cp310-win_amd64.whl (26.2 MB)\n",
      "   ---------------------------------------- 0.0/26.2 MB ? eta -:--:--\n",
      "   ---------------------------------------  26.2/26.2 MB 238.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.2/26.2 MB 87.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-21.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 pytz pandas aws_lambda_powertools pyodbc pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2bf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"level\":\"INFO\",\"location\":\"<module>:1228\",\"message\":\"================================================================================\",\"timestamp\":\"2025-08-01 02:19:33,264+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"<module>:1230\",\"message\":\"Version: SQL Server Identifier Parsing Fix v2.0\",\"timestamp\":\"2025-08-01 02:19:33,268+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"<module>:1231\",\"message\":\"Table: M_PAIS\",\"timestamp\":\"2025-08-01 02:19:33,270+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"<module>:1232\",\"message\":\"EndPoint: PEBDDATA2\",\"timestamp\":\"2025-08-01 02:19:33,271+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"<module>:1233\",\"message\":\"================================================================================\",\"timestamp\":\"2025-08-01 02:19:33,273+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"<module>:1235\",\"message\":\"Starting data extraction process\",\"timestamp\":\"2025-08-01 02:19:33,275+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"<module>:1236\",\"message\":\"Configuration: {'S3_RAW_PREFIX': 's3://sofia-dev-datalake-510543735161-us-east-1-raw-s3/', 'ARN_TOPIC_FAILED': 'arn:aws:sns:us-east-1:510543735161:sofia-dev-datalake-failed-sns', 'ARN_TOPIC_SUCCESS': 'arn:aws:sns:us-east-1:510543735161:sofia-dev-datalake-success-sns', 'PROJECT_NAME': 'datalake', 'TEAM': 'apdayc', 'DATA_SOURCE': 'bigmagic', 'ENVIRONMENT': 'DEV', 'REGION': 'us-east-1', 'DYNAMO_LOGS_TABLE': 'sofia-dev-datalake-logs-ddb', 'ENDPOINT_NAME': 'PEBDDATA2', 'TABLE_NAME': 'M_PAIS', 'TABLES_CSV_S3': '../../artifacts/configuration/csv/tables.csv', 'CREDENTIALS_CSV_S3': '../../artifacts/configuration/csv/credentials.csv', 'COLUMNS_CSV_S3': '../../artifacts/configuration/csv/columns.csv'}\",\"timestamp\":\"2025-08-01 02:19:33,276+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n"
     ]
    }
   ],
   "source": [
    "# Import required modules\n",
    "import concurrent.futures as futures\n",
    "import datetime as dt\n",
    "import calendar\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import uuid\n",
    "import boto3\n",
    "import pytz\n",
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "import gzip\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "IS_AWS_GLUE = False\n",
    "IS_AWS_S3 = False\n",
    "PARQUET_AVAILABLE = True\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    " \n",
    "# Import aje_libs \n",
    "try:\n",
    "    import aje_libs\n",
    "except ImportError as e:\n",
    "    # Search for aje_libs.zip file and extract it\n",
    "    import glob\n",
    "    import zipfile\n",
    "    \n",
    "    search_paths = ['/tmp', '/tmp/glue-python-libs-*', '/glue/lib']\n",
    "    for search_pattern in search_paths:\n",
    "        for base_path in glob.glob(search_pattern):\n",
    "            if os.path.isdir(base_path):\n",
    "                # Look for aje_libs.zip file\n",
    "                zip_file = os.path.join(base_path, 'aje_libs.zip')\n",
    "                if os.path.exists(zip_file):\n",
    "                    # Extract the zip file\n",
    "                    extract_path = os.path.join(base_path, 'extracted')\n",
    "                    try:\n",
    "                        os.makedirs(extract_path, exist_ok=True)\n",
    "                        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                            zip_ref.extractall(extract_path)\n",
    "                        \n",
    "                        # Add extract path to sys.path if aje_libs is there\n",
    "                        if 'aje_libs' in os.listdir(extract_path):\n",
    "                            sys.path.insert(0, extract_path)\n",
    "                            break\n",
    "                            \n",
    "                    except Exception as extract_err:\n",
    "                        pass\n",
    "    \n",
    "    # Try importing again\n",
    "    try:\n",
    "        import aje_libs\n",
    "    except ImportError as e2:\n",
    "        pass\n",
    "\n",
    "# Import custom helpers\n",
    "from aje_libs.common.logger import custom_logger, set_logger_config\n",
    "from aje_libs.common.helpers.dynamodb_helper import DynamoDBHelper\n",
    "from aje_libs.common.helpers.s3_helper import S3Helper\n",
    "from aje_libs.common.helpers.secrets_helper import SecretsHelper\n",
    "from aje_libs.bd.helpers.datafactory_helper import DatabaseFactoryHelper\n",
    "\n",
    "# Setup timezone and date variables\n",
    "TZ_LIMA = pytz.timezone('America/Lima')\n",
    "YEARS_LIMA = dt.datetime.now(TZ_LIMA).strftime('%Y')\n",
    "MONTHS_LIMA = dt.datetime.now(TZ_LIMA).strftime('%m')\n",
    "DAYS_LIMA = dt.datetime.now(TZ_LIMA).strftime('%d')\n",
    "NOW_LIMA = dt.datetime.now(pytz.utc).astimezone(TZ_LIMA)\n",
    "\n",
    "class DataExtractor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.logger = custom_logger(__name__)\n",
    "        \n",
    "        # Initialize DynamoDB helpers\n",
    "        self.dynamo_logs_table = self.config['DYNAMO_LOGS_TABLE'].strip()\n",
    "        self.logs_table_db = DynamoDBHelper(self.dynamo_logs_table, \"PROCESS_ID\", None)\n",
    "        self.team = self.config.get('TEAM')\n",
    "        self.data_source = self.config.get('DATA_SOURCE')\n",
    "        self.environment = self.config.get('ENVIRONMENT')\n",
    "        self.project_name = self.config.get('PROJECT_NAME')\n",
    "        self.region = self.config.get('REGION')\n",
    "        self.table_name = self.config.get('TABLE_NAME')\n",
    "        \n",
    "        # Initialize S3 helper\n",
    "        self.s3_helper = S3Helper(self.config['S3_RAW_PREFIX'].split(\"/\")[2])\n",
    "        \n",
    "        # Load configuration from CSV files in S3\n",
    "        self._load_csv_configurations()\n",
    "        \n",
    "        # Initialize table data and endpoint data\n",
    "        self._initialize_data()\n",
    "        \n",
    "    def _load_csv_configurations(self):\n",
    "        \"\"\"Load configuration from CSV files in S3\"\"\"\n",
    "        try:\n",
    "            import csv\n",
    "            from io import StringIO\n",
    "            def load_csv_from_s3(s3_path):\n",
    "                \"\"\"Load CSV file from S3 and return as list of dictionaries\"\"\"\n",
    "                s3_client = boto3.client('s3')\n",
    "                bucket = s3_path.split('/')[2]\n",
    "                key = '/'.join(s3_path.split('/')[3:])\n",
    "                \n",
    "                response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "                content = response['Body'].read().decode('latin-1')\n",
    "                \n",
    "                csv_data = []\n",
    "                reader = csv.DictReader(StringIO(content), delimiter=';')\n",
    "                for row in reader:\n",
    "                    csv_data.append(row)\n",
    "                \n",
    "                return csv_data\n",
    "            \n",
    "            def load_csv_from_local(file_path):\n",
    "                import csv\n",
    "                \"\"\"Carga un archivo CSV local y lo devuelve como lista de diccionarios\"\"\"\n",
    "                csv_data = []\n",
    "                with open(file_path, mode='r', encoding='latin-1') as file:\n",
    "                    reader = csv.DictReader(file, delimiter=';')\n",
    "                    for row in reader:\n",
    "                        csv_data.append(row)\n",
    "                return csv_data\n",
    "            \n",
    "            # Load CSV data\n",
    "            self.logger.info(f\"TABLES_CSV_S3: {self.config['TABLES_CSV_S3']}\")\n",
    "            tables_data = load_csv_from_s3(self.config['TABLES_CSV_S3']) if IS_AWS_S3 else load_csv_from_local(self.config['TABLES_CSV_S3'])\n",
    "            self.logger.info(f\"tables_data: {tables_data[0]}\")\n",
    "            self.logger.info(f\"CREDENTIALS_CSV_S3: {self.config['CREDENTIALS_CSV_S3']}\")\n",
    "            credentials_data = load_csv_from_s3(self.config['CREDENTIALS_CSV_S3']) if IS_AWS_S3 else load_csv_from_local(self.config['CREDENTIALS_CSV_S3'])\n",
    "            self.logger.info(f\"credentials_data: {credentials_data[0]}\")\n",
    "            self.logger.info(f\"COLUMNS_CSV_S3: {self.config['COLUMNS_CSV_S3']}\")\n",
    "            columns_data = load_csv_from_s3(self.config['COLUMNS_CSV_S3']) if IS_AWS_S3 else load_csv_from_local(self.config['COLUMNS_CSV_S3'])\n",
    "            self.logger.info(f\"columns_data: {columns_data[0]}\")\n",
    "\n",
    "            self.logger.info(f\"config: {self.config}\")\n",
    "\n",
    "            # Filter data for current table and database\n",
    "            table_name = self.config.get('TABLE_NAME')\n",
    "            endpoint_name = self.config.get('ENDPOINT_NAME')\n",
    "            environment = self.config.get('ENVIRONMENT')\n",
    "            \n",
    "            # Find table configuration\n",
    "            self.table_data = None\n",
    "            for row in tables_data:\n",
    "                if row.get('STAGE_TABLE_NAME', '').upper() == table_name.upper():\n",
    "                    self.table_data = row\n",
    "                    self.logger.info(f\"Found table configuration for {table_name}\")\n",
    "                    break\n",
    "            \n",
    "            if not self.table_data:\n",
    "                raise Exception(f\"Table configuration not found for {table_name}\")\n",
    "            \n",
    "            # Process the COLUMNS field to handle potential SQL Server identifier length issues\n",
    "            self._process_columns_field()\n",
    "            \n",
    "            # Find database credentials\n",
    "            self.endpoint_data = None\n",
    "            for row in credentials_data:\n",
    "                if (row.get('ENDPOINT_NAME', '') == endpoint_name and \n",
    "                    row.get('ENV', '').upper() == environment.upper()):\n",
    "                    self.endpoint_data = row\n",
    "                    break\n",
    "            \n",
    "            if not self.endpoint_data:\n",
    "                raise Exception(f\"Database credentials not found for {src_db_name} in {environment}\")\n",
    "            \n",
    "            # Add ENDPOINT_NAME for compatibility\n",
    "            self.endpoint_data['ENDPOINT_NAME'] = self.endpoint_data.get('SRC_DB_NAME', '')\n",
    "            \n",
    "            # Apply old logic to determine LOAD_TYPE if not explicitly set\n",
    "            if not self.table_data.get('LOAD_TYPE') or self.table_data.get('LOAD_TYPE', '').strip() == '':\n",
    "                if self.table_data.get('SOURCE_TABLE_TYPE', '') == 't':\n",
    "                    if self.endpoint_data.get('ENDPOINT_NAME', '') == 'SALESFORCE_ING':\n",
    "                        self.table_data['LOAD_TYPE'] = 'days_off'\n",
    "                        self.table_data['NUM_DAYS'] = '10'\n",
    "                    else:\n",
    "                        self.table_data['LOAD_TYPE'] = 'incremental'\n",
    "                else:\n",
    "                    self.table_data['LOAD_TYPE'] = 'full'\n",
    "            \n",
    "            self.logger.info(f\"Determined LOAD_TYPE: {self.table_data.get('LOAD_TYPE', 'not set')} for table {table_name}\")\n",
    "            \n",
    "            # Filter columns for current table\n",
    "            self.columns_metadata = []\n",
    "            for row in columns_data:\n",
    "                if row.get('TABLE_NAME', '').upper() == table_name.upper():\n",
    "                    self.columns_metadata.append(row)\n",
    "            \n",
    "            self.logger.info(f\"Loaded configuration for table: {table_name}\")\n",
    "            self.logger.info(f\"Columns count: {len(self.columns_metadata)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading CSV configurations: {str(e)}\")\n",
    "            self._log_error(str(e))\n",
    "            raise Exception(f\"Failed to load CSV configurations: {str(e)}\")\n",
    "        \n",
    "    def _load_json_configurations(self):\n",
    "        \"\"\"Load configuration from JSON parameters passed by CDK (DEPRECATED)\"\"\"\n",
    "        try:\n",
    "            import json\n",
    "            \n",
    "            # Parse JSON configurations from job parameters\n",
    "            self.table_data = json.loads(self.config['TABLE_CONFIG'])\n",
    "            self.endpoint_data = json.loads(self.config['DB_CONFIG'])\n",
    "            self.columns_metadata = json.loads(self.config['COLUMNS_CONFIG'])\n",
    "            \n",
    "            self.logger.info(f\"Loaded configuration for table: {self.table_name}\")\n",
    "            self.logger.info(f\"Columns count: {len(self.columns_metadata)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading JSON configurations: {str(e)}\")\n",
    "            self._log_error(str(e))\n",
    "            raise Exception(f\"Failed to load JSON configurations: {str(e)}\")\n",
    "        \n",
    "    def _initialize_data(self):\n",
    "        \"\"\"Initialize table and endpoint data from loaded CSV configurations\"\"\"\n",
    "        try:\n",
    "            # Set S3 path using loaded data\n",
    "            team = self.team\n",
    "            data_source = self.data_source\n",
    "            endpoint_name = self.endpoint_data['ENDPOINT_NAME']\n",
    "            # Get clean table name (remove alias after space) for S3 path\n",
    "            clean_table_name = self._get_clean_table_name()\n",
    "            self.day_route = f\"{team}/{data_source}/{endpoint_name}/{clean_table_name}/year={YEARS_LIMA}/month={MONTHS_LIMA}/day={DAYS_LIMA}/\"\n",
    "            \n",
    "            self.s3_raw_path = self.config['S3_RAW_PREFIX'] + self.day_route\n",
    "            self.bucket = self.config['S3_RAW_PREFIX'].split(\"/\")[2]\n",
    "            \n",
    "            # Initialize database connection\n",
    "            self.init_db_connection()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error while searching for table data\")\n",
    "            self.logger.error(e)\n",
    "            self._log_error(str(e))\n",
    "            raise Exception(f\"Failed to initialize data: {str(e)}\")\n",
    "    \n",
    "    def init_db_connection(self):\n",
    "        \"\"\"Initialize database connection using DatabaseFactoryHelper\"\"\"\n",
    "        try:\n",
    "            # Get credentials using SecretsHelper\n",
    "            self.secrets_helper = SecretsHelper(f\"{self.config['ENVIRONMENT']}/{self.config['PROJECT_NAME']}/{self.team}/{self.data_source}\".lower())\n",
    "            password = self.secrets_helper.get_secret_value(self.endpoint_data[\"SRC_DB_SECRET\"])\n",
    "            \n",
    "            # Record connection information for logging\n",
    "            self.db_type = self.endpoint_data['BD_TYPE']\n",
    "            self.server = self.endpoint_data['SRC_SERVER_NAME']\n",
    "            self.port = self.endpoint_data['DB_PORT_NUMBER']\n",
    "            self.db_name = self.endpoint_data['SRC_DB_NAME']\n",
    "            self.username = self.endpoint_data['SRC_DB_USERNAME']\n",
    "            \n",
    "            # Additional params based on database type\n",
    "            additional_params = {}\n",
    "            \n",
    "            if self.db_type == 'oracle':\n",
    "                additional_params['service_name'] = self.db_name\n",
    "                self.url = f\"{self.server}:{self.port}/{self.db_name}\"\n",
    "                self.driver = \"oracle.jdbc.driver.OracleDriver\"\n",
    "            elif self.db_type == 'mssql':\n",
    "                self.url = f\"{self.server}:{self.port}\"\n",
    "                self.driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "            elif self.db_type == 'mysql':\n",
    "                additional_params['charset'] = 'utf8mb4'\n",
    "                self.url = f\"{self.server}:{self.port}\"\n",
    "                self.driver = \"com.mysql.cj.jdbc.Driver\"\n",
    "            \n",
    "            # Create database helper using factory\n",
    "            self.db_helper = DatabaseFactoryHelper.create_helper(\n",
    "                db_type=self.db_type,\n",
    "                server=self.server,\n",
    "                database=self.db_name,\n",
    "                username=self.username,\n",
    "                password=password,\n",
    "                port=int(self.port) if self.port else None,\n",
    "                **additional_params\n",
    "            )\n",
    "            self.logger.info(f\"server: {self.server}\")\n",
    "            self.logger.info(f\"port: {self.port}\")\n",
    "            self.logger.info(f\"db_name: {self.db_name}\")\n",
    "            self.logger.info(f\"username: {self.username}\")\n",
    "            self.logger.info(f\"password: {password}\")\n",
    "            self.logger.info(f\"Database connection initialized for {self.db_type} database\")\n",
    "            self.logger.info(f\"driver: {self.driver}\")\n",
    "            self.logger.info(f\"url: {self.url}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error initializing database connection: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def _get_clean_table_name(self):\n",
    "        \"\"\"\n",
    "        Extract clean table name from SOURCE_TABLE, removing alias after space.\n",
    "        For Ingest Bigmagic, the structure is: [sourcetablename] [alias] (alias is optional)\n",
    "        \n",
    "        Examples:\n",
    "        - \"mcompa1f m\" -> \"mcompa1f\"\n",
    "        - \"users u\" -> \"users\"\n",
    "        - \"simple_table\" -> \"simple_table\"\n",
    "        \"\"\"\n",
    "        source_table = self.table_data.get('SOURCE_TABLE', self.table_name)\n",
    "        # Split by space and take only the first part (table name)\n",
    "        clean_name = source_table.split()[0] if source_table and ' ' in source_table else source_table\n",
    "        self.logger.info(f\"Clean table name extracted: '{source_table}' -> '{clean_name}'\")\n",
    "        return clean_name\n",
    "    \n",
    "    def _log_error(self, error_message):\n",
    "        \"\"\"Log error to DynamoDB and update table status\"\"\"\n",
    "        try:\n",
    "            # Send error message via SNS if topic ARN is available\n",
    "            if 'TOPIC_ARN' in self.config and self.config['TOPIC_ARN']:\n",
    "                self.send_error_message(self.config['TOPIC_ARN'], self.table_data.get('TARGET_TABLE_NAME', self.table_name), error_message)\n",
    "            \n",
    "            # Add log entry to DynamoDB logs table\n",
    "            clean_table_name = self._get_clean_table_name()\n",
    "            process_id = f\"DLB_{self.table_name.split('_')[0]}_{clean_table_name}_{NOW_LIMA.strftime('%Y%m%d_%H%M%S')}\"\n",
    "            log = {\n",
    "                'PROCESS_ID': process_id,\n",
    "                'DATE_SYSTEM': NOW_LIMA.strftime('%Y%m%d_%H%M%S'),\n",
    "                'PROJECT_NAME': self.config['PROJECT_NAME'],\n",
    "                'FLOW_NAME': 'extract_bigmagic',\n",
    "                'TASK_NAME': 'extract_table_bigmagic',\n",
    "                'TASK_STATUS': 'error',\n",
    "                'MESSAGE': error_message,\n",
    "                'PROCESS_TYPE': 'D' if self.table_data.get('LOAD_TYPE', 'full').strip() in ['incremental'] else 'F',\n",
    "                'CONTEXT': f\"{{server='[{self.endpoint_data['ENDPOINT_NAME']},{self.endpoint_data['SRC_SERVER_NAME']}]', user='{self.endpoint_data['SRC_DB_USERNAME']}', table='{self.table_data.get('SOURCE_TABLE', self.table_name)}'}}\"\n",
    "            }\n",
    "            self.logs_table_db.put_item(log)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to log error: {str(e)}\")\n",
    "    \n",
    "    def _log_success(self):\n",
    "        \"\"\"Log success to DynamoDB and update table status\"\"\"\n",
    "        try:\n",
    "            # Add log entry to DynamoDB logs table\n",
    "            clean_table_name = self._get_clean_table_name()\n",
    "            process_id = f\"DLB_{self.table_name.split('_')[0]}_{clean_table_name}_{NOW_LIMA.strftime('%Y%m%d_%H%M%S')}\"\n",
    "            self.logger.info(f\"process_id: {process_id}\")\n",
    "            log = {\n",
    "                'PROCESS_ID': process_id,\n",
    "                'DATE_SYSTEM': NOW_LIMA.strftime('%Y%m%d_%H%M%S'),\n",
    "                'PROJECT_NAME': self.config['PROJECT_NAME'],\n",
    "                'FLOW_NAME': 'extract_bigmagic',\n",
    "                'TASK_NAME': 'extract_table_bigmagic',\n",
    "                'TASK_STATUS': 'satisfactorio',\n",
    "                'MESSAGE': '',\n",
    "                'PROCESS_TYPE': 'D' if self.table_data.get('LOAD_TYPE', 'full').strip() in ['incremental'] else 'F',\n",
    "                'CONTEXT': f\"{{server='[{self.endpoint_data['ENDPOINT_NAME']},{self.endpoint_data['SRC_SERVER_NAME']}]', user='{self.endpoint_data['SRC_DB_USERNAME']}', table='{self.table_data.get('SOURCE_TABLE', self.table_name)}'}}\"\n",
    "            }\n",
    "            self.logs_table_db.put_item(log)\n",
    "            self.logger.info(\"DynamoDB updated with success status\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to log success: {str(e)}\")\n",
    "    \n",
    "    def send_error_message(self, topic_arn, table_name, error):\n",
    "        \"\"\"Send error message via SNS\"\"\"\n",
    "        client = boto3.client(\"sns\")\n",
    "        response = client.publish(\n",
    "            TopicArn=topic_arn,\n",
    "            Message=f\"Failed table: {table_name} \\nStep: raw job \\nLog ERROR : {error}\"\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def delete_from_target(self, bucket, s3_raw_path):\n",
    "        \"\"\"Delete objects from S3 bucket with specified prefix\"\"\"\n",
    "        try:\n",
    "            # Use S3Helper to manage deletions\n",
    "            objects_to_delete = self.s3_helper.list_objects(prefix=s3_raw_path)\n",
    "            if objects_to_delete:\n",
    "                keys_to_delete = [obj.get('Key') for obj in objects_to_delete]\n",
    "                self.s3_helper.delete_objects(keys_to_delete)\n",
    "                self.logger.info(f\"Deleted {len(keys_to_delete)} objects from {s3_raw_path}\")\n",
    "            else:\n",
    "                self.logger.info(f\"No objects found to delete in {s3_raw_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error deleting objects from S3: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "    def transform_to_dt(self, date):\n",
    "        \"\"\"Convert string date to datetime object\"\"\"\n",
    "        start_dt = dt.datetime(\n",
    "            year=int(date[:4]),\n",
    "            month=int(date[5:7]),\n",
    "            day=int(date[8:10]),\n",
    "            hour=int(date[11:13]),\n",
    "            minute=int(date[14:16]),\n",
    "            second=int(date[17:19])\n",
    "        )\n",
    "        return start_dt\n",
    "\n",
    "    def get_limits_for_filter(self, month_diff, data_type):\n",
    "        \"\"\"Get lower and upper limits for date filters based on data type\"\"\"\n",
    "        data_type = data_type.strip()\n",
    "        upper_limit = dt.datetime.now(TZ_LIMA)\n",
    "        lower_limit = upper_limit - relativedelta(months=(-1*int(month_diff)))\n",
    "        \n",
    "        if data_type == \"aje_period\":\n",
    "            return lower_limit.strftime('%Y%m'), upper_limit.strftime('%Y%m')\n",
    "        \n",
    "        elif data_type == \"aje_date\":\n",
    "            _, last_day = calendar.monthrange(upper_limit.year, upper_limit.month)\n",
    "            upper_limit = upper_limit.replace(day=last_day, tzinfo=TZ_LIMA)\n",
    "            lower_limit = lower_limit.replace(day=1, tzinfo=TZ_LIMA)\n",
    "            upper_limit = (upper_limit - dt.datetime(1900, 1, 1, tzinfo=TZ_LIMA)).days + 693596\n",
    "            lower_limit = (lower_limit - dt.datetime(1900, 1, 1, tzinfo=TZ_LIMA)).days + 693596\n",
    "            return str(lower_limit), str(upper_limit)\n",
    "            \n",
    "        elif data_type == \"aje_processperiod\":\n",
    "            _, last_day = calendar.monthrange(upper_limit.year, upper_limit.month)\n",
    "            upper_limit = upper_limit.replace(day=last_day, tzinfo=TZ_LIMA)\n",
    "            lower_limit = lower_limit.replace(day=1, tzinfo=TZ_LIMA)\n",
    "            upper_limit = (upper_limit - dt.datetime(1900, 1, 1, tzinfo=TZ_LIMA)).days + 693596\n",
    "            lower_limit = (lower_limit - dt.datetime(1900, 1, 1, tzinfo=TZ_LIMA)).days + 693596\n",
    "            return str(lower_limit), str(upper_limit)\n",
    "      \n",
    "        return lower_limit.strftime('%Y%m'), upper_limit.strftime('%Y%m')\n",
    "\n",
    "    def execute_db_query(self, query):\n",
    "        \"\"\"Execute query on the database and return results as DataFrame\"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Executing query on database: {query}\")\n",
    "            # Use the database helper created by the factory\n",
    "            if hasattr(self.db_helper, 'execute_query_as_dataframe'):\n",
    "                # Directly use dataframe if supported\n",
    "                return self.db_helper.execute_query_as_dataframe(query)\n",
    "            else:\n",
    "                # Otherwise, convert dict results to DataFrame\n",
    "                result = self.db_helper.execute_query_as_dict(query)\n",
    "                return pd.DataFrame(result)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error executing database query: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def extract_columns_from_query(self, query):\n",
    "        \"\"\"Extract column names from SELECT statement - handles complex expressions, functions, and aliases\"\"\"\n",
    "        try:\n",
    "            # Remove extra whitespaces but preserve original case for column extraction\n",
    "            clean_query = ' '.join(query.strip().split())\n",
    "            \n",
    "            # Find SELECT and FROM positions (case insensitive)\n",
    "            select_match = re.search(r'\\bSELECT\\s+', clean_query, re.IGNORECASE)\n",
    "            from_match = re.search(r'\\bFROM\\s+', clean_query, re.IGNORECASE)\n",
    "            \n",
    "            if not select_match or not from_match:\n",
    "                raise ValueError(\"Could not parse SELECT statement\")\n",
    "            \n",
    "            # Extract the column part (preserve original case)\n",
    "            select_start = select_match.end()\n",
    "            from_start = from_match.start()\n",
    "            columns_part = clean_query[select_start:from_start].strip()\n",
    "            \n",
    "            # Smart split by comma (respecting parentheses, quotes, and string concatenation)\n",
    "            column_expressions = self._smart_split_columns(columns_part)\n",
    "            \n",
    "            columns = []\n",
    "            for expr in column_expressions:\n",
    "                expr = expr.strip()\n",
    "                column_name = self._extract_column_alias_or_name(expr)\n",
    "                if column_name:\n",
    "                    columns.append(column_name)\n",
    "            \n",
    "            return columns\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Could not extract columns from query: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _smart_split_columns(self, columns_part):\n",
    "        \"\"\"Split columns by comma, respecting parentheses, quotes, and operators\"\"\"\n",
    "        expressions = []\n",
    "        current_expr = \"\"\n",
    "        paren_count = 0\n",
    "        in_single_quote = False\n",
    "        in_double_quote = False\n",
    "        i = 0\n",
    "        \n",
    "        while i < len(columns_part):\n",
    "            char = columns_part[i]\n",
    "            \n",
    "            # Handle quotes\n",
    "            if char == \"'\" and not in_double_quote:\n",
    "                in_single_quote = not in_single_quote\n",
    "            elif char == '\"' and not in_single_quote:\n",
    "                in_double_quote = not in_double_quote\n",
    "            \n",
    "            # Handle parentheses (only when not in quotes)\n",
    "            elif char == '(' and not in_single_quote and not in_double_quote:\n",
    "                paren_count += 1\n",
    "            elif char == ')' and not in_single_quote and not in_double_quote:\n",
    "                paren_count -= 1\n",
    "            \n",
    "            # Handle comma separation (only when not in quotes and parentheses are balanced)\n",
    "            elif (char == ',' and paren_count == 0 and \n",
    "                not in_single_quote and not in_double_quote):\n",
    "                if current_expr.strip():\n",
    "                    expressions.append(current_expr.strip())\n",
    "                current_expr = \"\"\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            current_expr += char\n",
    "            i += 1\n",
    "        \n",
    "        # Add the last expression\n",
    "        if current_expr.strip():\n",
    "            expressions.append(current_expr.strip())\n",
    "        \n",
    "        return expressions\n",
    "\n",
    "    def _extract_column_alias_or_name(self, expression):\n",
    "        \"\"\"Extract column alias or name from a column expression\"\"\"\n",
    "        try:\n",
    "            expr = expression.strip()\n",
    "            \n",
    "            # Method 1: Check for explicit AS alias (case insensitive)\n",
    "            as_match = re.search(r'\\s+AS\\s+([a-zA-Z_][a-zA-Z0-9_]*)\\s*$', expr, re.IGNORECASE)\n",
    "            if as_match:\n",
    "                return as_match.group(1).strip()\n",
    "            \n",
    "            # Method 2: Check for implicit alias (space-separated, not within functions)\n",
    "            # Only if the expression contains functions, operators, or complex expressions\n",
    "            if any(indicator in expr.lower() for indicator in ['(', '+', '-', '*', '/', 'ltrim', 'rtrim', 'convert', 'cast', 'dbo.', 'case']):\n",
    "                # Look for implicit alias at the end\n",
    "                # Split by spaces and get the last part that looks like a column name\n",
    "                words = expr.split()\n",
    "                if len(words) >= 2:\n",
    "                    potential_alias = words[-1].strip()\n",
    "                    # Check if it's a valid identifier (not a keyword or operator)\n",
    "                    if (re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', potential_alias) and \n",
    "                        potential_alias.lower() not in ['and', 'or', 'not', 'in', 'like', 'is', 'null', 'from', 'where', 'select']):\n",
    "                        return potential_alias\n",
    "            \n",
    "            # Method 3: For simple column names (no functions or operators)\n",
    "            if not any(indicator in expr.lower() for indicator in ['(', '+', '-', '*', '/', 'ltrim', 'rtrim', 'convert', 'cast', 'dbo.', 'case', \"'\", '\"']):\n",
    "                # It's a simple column name, clean it up\n",
    "                clean_name = expr.strip('[]\"`').strip()\n",
    "                if re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', clean_name):\n",
    "                    return clean_name\n",
    "            \n",
    "            # Method 4: Try to extract the most relevant column name from complex expressions\n",
    "            # Look for column names in the expression (exclude function names)\n",
    "            column_pattern = r'\\b([a-zA-Z_][a-zA-Z0-9_]*)\\b'\n",
    "            potential_columns = re.findall(column_pattern, expr)\n",
    "            \n",
    "            if potential_columns:\n",
    "                # Filter out known function names and keywords\n",
    "                excluded_words = {\n",
    "                    'ltrim', 'rtrim', 'convert', 'cast', 'case', 'when', 'then', 'else', 'end',\n",
    "                    'dbo', 'func_cas_todatetime', 'select', 'from', 'where', 'and', 'or', 'in',\n",
    "                    'like', 'is', 'null', 'not', 'between', 'exists', 'as', 'varchar', 'int',\n",
    "                    'datetime', 'char', 'nvarchar', 'decimal', 'float', 'bit'\n",
    "                }\n",
    "                \n",
    "                filtered_columns = [col for col in potential_columns \n",
    "                                if col.lower() not in excluded_words]\n",
    "                \n",
    "                if filtered_columns:\n",
    "                    # Return the first meaningful column name found\n",
    "                    return filtered_columns[0]\n",
    "            \n",
    "            # Method 5: Last resort - generate a name based on the expression\n",
    "            if 'ltrim' in expr.lower() and 'rtrim' in expr.lower() and '+' in expr:\n",
    "                return 'concatenated_field'\n",
    "            elif 'func_cas_todatetime' in expr.lower():\n",
    "                return 'datetime_field'\n",
    "            elif 'convert' in expr.lower() or 'cast' in expr.lower():\n",
    "                return 'converted_field'\n",
    "            else:\n",
    "                return f'expr_field_{hash(expr) % 1000}'\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Could not extract column name from expression '{expression}': {e}\")\n",
    "            return f'unknown_col_{hash(expression) % 1000}'\n",
    "\n",
    "    def extract_columns_from_query_specific(self, query):\n",
    "        \"\"\"Extract columns specifically designed for your query pattern\"\"\"\n",
    "        try:\n",
    "            # Para tu query específico, puedes usar regex más específicos\n",
    "            specific_patterns = [\n",
    "                # Para: ltrim(rtrim(compania)) + '|' + ltrim(rtrim(transport)) as id\n",
    "                (r\"ltrim\\(rtrim\\(compania\\)\\)\\s*\\+.*?ltrim\\(rtrim\\(transport\\)\\)\\s+as\\s+(\\w+)\", 1),\n",
    "                # Para: dbo.func_cas_todatetime(fecultimod,horultimod) lastmodifydate\n",
    "                (r\"dbo\\.func_cas_todatetime\\([^)]+\\)\\s+(\\w+)\", 1),\n",
    "                # Para columnas simples con o sin alias\n",
    "                (r\"\\b(\\w+)\\s*(?:,|$|\\s+from)\", 1),\n",
    "            ]\n",
    "            \n",
    "            columns = []\n",
    "            remaining_query = query\n",
    "            \n",
    "            for pattern, group_idx in specific_patterns:\n",
    "                matches = re.finditer(pattern, remaining_query, re.IGNORECASE)\n",
    "                for match in matches:\n",
    "                    column_name = match.group(group_idx)\n",
    "                    if column_name and column_name not in columns:\n",
    "                        columns.append(column_name)\n",
    "                    # Remove the matched part to avoid double-counting\n",
    "                    remaining_query = remaining_query.replace(match.group(0), '', 1)\n",
    "            \n",
    "            # Si no se encontraron columnas con patrones específicos, usar el método genérico\n",
    "            if not columns:\n",
    "                return self.extract_columns_from_query(query)\n",
    "            \n",
    "            return columns\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Specific column extraction failed: {e}\")\n",
    "            return self.extract_columns_from_query(query)\n",
    "\n",
    "    def write_dataframe_to_s3_parquet(self, df, s3_path, filename=None):\n",
    "        \"\"\"Write pandas DataFrame to S3 in Parquet format\"\"\"\n",
    "        try:\n",
    "            s3_client = boto3.client('s3')\n",
    "            \n",
    "            # Parse S3 path\n",
    "            if s3_path.startswith('s3://'):\n",
    "                s3_path = s3_path[5:]  # Remove s3:// prefix\n",
    "            \n",
    "            path_parts = s3_path.split('/', 1)\n",
    "            bucket_name = path_parts[0]\n",
    "            key_prefix = path_parts[1] if len(path_parts) > 1 else ''\n",
    "            \n",
    "            # Generate filename if not provided\n",
    "            if not filename:\n",
    "                filename = f\"data_{uuid.uuid4().hex[:8]}.parquet\"\n",
    "            \n",
    "            # Ensure key_prefix ends with /\n",
    "            if key_prefix and not key_prefix.endswith('/'):\n",
    "                key_prefix += '/'\n",
    "            \n",
    "            parquet_key = key_prefix + filename\n",
    "            \n",
    "            # Write as Parquet format\n",
    "            parquet_buffer = io.BytesIO()\n",
    "            df.to_parquet(parquet_buffer, index=False, engine='pyarrow')\n",
    "            parquet_bytes = parquet_buffer.getvalue()\n",
    "             \n",
    "            if not parquet_key.endswith('.parquet'):\n",
    "                parquet_key += '.parquet'\n",
    "            \n",
    "            # Upload to S3\n",
    "            s3_client.put_object(\n",
    "                Bucket=bucket_name,\n",
    "                Key=parquet_key,\n",
    "                Body=parquet_bytes,\n",
    "                ContentType='application/octet-stream'\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f\"Successfully wrote DataFrame to s3://{bucket_name}/{parquet_key} as Parquet\")\n",
    "            return f\"s3://{bucket_name}/{parquet_key}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error writing DataFrame to S3 as Parquet: {str(e)}\") \n",
    "\n",
    "    def _process_columns_field(self):\n",
    "        \"\"\"Process the COLUMNS field to handle potential SQL Server identifier length issues\"\"\"\n",
    "        try:\n",
    "            if not self.table_data or 'COLUMNS' not in self.table_data:\n",
    "                self.logger.info(\"No COLUMNS field found in table_data\")\n",
    "                return\n",
    "                \n",
    "            columns_str = self.table_data['COLUMNS']\n",
    "            if not columns_str or columns_str.strip() == '':\n",
    "                self.logger.info(\"COLUMNS field is empty or None\")\n",
    "                return\n",
    "                \n",
    "            self.logger.info(f\"Processing COLUMNS field for table: {self.table_data.get('STAGE_TABLE_NAME', 'UNKNOWN')}\")\n",
    "                \n",
    "            # Split columns by comma, but be careful with complex expressions containing commas\n",
    "            columns = []\n",
    "            current_column = \"\"\n",
    "            paren_count = 0\n",
    "            quote_count = 0\n",
    "            \n",
    "            for char in columns_str:\n",
    "                current_column += char\n",
    "                if char == '(' and quote_count % 2 == 0:\n",
    "                    paren_count += 1\n",
    "                elif char == ')' and quote_count % 2 == 0:\n",
    "                    paren_count -= 1\n",
    "                elif char == \"'\":\n",
    "                    quote_count += 1\n",
    "                elif char == ',' and paren_count == 0 and quote_count % 2 == 0:\n",
    "                    columns.append(current_column[:-1].strip())  # Remove the comma\n",
    "                    current_column = \"\"\n",
    "            \n",
    "            # Add the last column\n",
    "            if current_column.strip():\n",
    "                columns.append(current_column.strip())\n",
    "            \n",
    "            # Process each individual column - keep all as-is (no truncation)\n",
    "            processed_columns = []\n",
    "            \n",
    "            for i, col in enumerate(columns):\n",
    "                col = col.strip()\n",
    "                if not col:\n",
    "                    continue\n",
    "                processed_columns.append(col)\n",
    "            \n",
    "            # Update the table data with processed columns\n",
    "            result = ', '.join(processed_columns)\n",
    "            self.table_data['COLUMNS'] = result\n",
    "            \n",
    "            self.logger.info(f\"Processed {len(columns)} columns successfully\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ERROR in _process_columns_field: {e}\")\n",
    "            self.logger.error(f\"Table: {self.table_data.get('STAGE_TABLE_NAME', 'UNKNOWN') if self.table_data else 'NO_TABLE_DATA'}\")\n",
    "            # Don't raise exception, just log the error and continue with original data\n",
    "\n",
    "    def _apply_column_processing_to_query(self, columns_str):\n",
    "        \"\"\"Apply column processing to any columns string to handle SQL Server identifier length limits\"\"\"\n",
    "        try:\n",
    "            if not columns_str or columns_str.strip() == '':\n",
    "                return columns_str\n",
    "                \n",
    "            # CRITICAL FIX: Remove problematic double quotes that cause SQL Server parsing issues\n",
    "            clean_columns = columns_str.strip()\n",
    "            \n",
    "            # Check for double quotes and remove them\n",
    "            double_quote_count = clean_columns.count('\"')\n",
    "            if double_quote_count > 0:\n",
    "                # CRITICAL: Check if the entire field is wrapped in quotes\n",
    "                if clean_columns.startswith('\"') and clean_columns.endswith('\"') and double_quote_count == 2:\n",
    "                    # Remove the wrapping quotes\n",
    "                    clean_columns = clean_columns[1:-1]\n",
    "                else:\n",
    "                    # REMOVE ALL DOUBLE QUOTES - they cause SQL Server to treat entire sections as single identifiers\n",
    "                    clean_columns = clean_columns.replace('\"', '')\n",
    "            \n",
    "            # Additional validation after quote removal\n",
    "            if clean_columns.strip() == '':\n",
    "                return columns_str  # Return original to avoid total failure\n",
    "            \n",
    "            # Step 2: Split columns more intelligently\n",
    "            # Use a simpler approach - split by comma but handle nested functions\n",
    "            columns = []\n",
    "            current_column = \"\"\n",
    "            paren_count = 0\n",
    "            quote_count = 0\n",
    "            in_single_quote = False\n",
    "            \n",
    "            i = 0\n",
    "            while i < len(clean_columns):\n",
    "                char = clean_columns[i]\n",
    "                current_column += char\n",
    "                \n",
    "                if char == \"'\" and not in_single_quote:\n",
    "                    in_single_quote = True\n",
    "                elif char == \"'\" and in_single_quote:\n",
    "                    in_single_quote = False\n",
    "                elif not in_single_quote:\n",
    "                    if char == '(':\n",
    "                        paren_count += 1\n",
    "                    elif char == ')':\n",
    "                        paren_count -= 1\n",
    "                    elif char == ',' and paren_count == 0:\n",
    "                        # This is a column separator\n",
    "                        column_text = current_column[:-1].strip()  # Remove the comma\n",
    "                        if column_text:\n",
    "                            columns.append(column_text)\n",
    "                        current_column = \"\"\n",
    "                \n",
    "                i += 1\n",
    "            \n",
    "            # Add the last column\n",
    "            if current_column.strip():\n",
    "                columns.append(current_column.strip())\n",
    "            \n",
    "            # Step 3: Keep all columns as-is (no truncation)\n",
    "            processed_columns = []\n",
    "            \n",
    "            for idx, col in enumerate(columns):\n",
    "                col = col.strip()\n",
    "                if not col:\n",
    "                    continue\n",
    "                \n",
    "                # Keep all columns as-is without any truncation\n",
    "                processed_columns.append(col)\n",
    "            \n",
    "            # Return processed columns\n",
    "            result = ', '.join(processed_columns)\n",
    "            return result\n",
    "                \n",
    "        except Exception as e:\n",
    "            return columns_str  # Return original if processing fails\n",
    "\n",
    "    def get_data(self, query, s3_raw_path, actual_thread, number_threads):\n",
    "        \"\"\"Get data from database and write to S3 as Parquet\"\"\"\n",
    "        try:\n",
    "            self.logger.info(query)\n",
    "            \n",
    "            # Execute query\n",
    "            df = self.execute_db_query(query)\n",
    "            \n",
    "            # Drop duplicates\n",
    "            df = df.drop_duplicates()\n",
    "            \n",
    "            # Write to S3 using Parquet format\n",
    "            if len(df) == 0:\n",
    "                # Try to get columns from the executed query first\n",
    "                if len(df.columns) > 0 and not all(col.startswith('Unnamed') for col in df.columns):\n",
    "                    columns = list(df.columns)\n",
    "                else:\n",
    "                    # Extract columns from the query itself\n",
    "                    columns = self.extract_columns_from_query(query)\n",
    "                    if not columns:\n",
    "                        # Fallback to specific extraction method\n",
    "                        columns = self.extract_columns_from_query_specific(query)\n",
    "                    \n",
    "                    if not columns:\n",
    "                        # Last resort: try structure query\n",
    "                        try:\n",
    "                            structure_query = f\"SELECT * FROM ({query}) AS subquery WHERE 1=0\"\n",
    "                            structure_df = self.execute_db_query(structure_query)\n",
    "                            columns = list(structure_df.columns)\n",
    "                        except:\n",
    "                            columns = ['unknown_column']\n",
    "                \n",
    "                # Create empty DataFrame with extracted columns\n",
    "                empty_df = pd.DataFrame(columns=columns)\n",
    "                self.write_dataframe_to_s3_parquet(\n",
    "                    empty_df, \n",
    "                    s3_raw_path, \n",
    "                    f\"empty_data_{actual_thread}.parquet\"\n",
    "                )\n",
    "                self.logger.info(f\"Written empty Parquet file with headers: {columns}\")\n",
    "            else:\n",
    "                # For non-empty dataframes, write the data as Parquet\n",
    "                filename = f\"data_thread_{actual_thread}_{uuid.uuid4().hex[:8]}.parquet\"\n",
    "                self.write_dataframe_to_s3_parquet(df, s3_raw_path, filename)\n",
    "            \n",
    "            self.logger.info(f\"finished thread n: {actual_thread}\")\n",
    "            self.logger.info(f\"Data sample: {df.head()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in get_data: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_min_max_values(self, partition_column):\n",
    "        \"\"\"Get min and max values for a partition column\"\"\"\n",
    "        try:\n",
    "            source_schema = self.table_data.get('SOURCE_SCHEMA', '')\n",
    "            source_table = self.table_data.get('SOURCE_TABLE', '')\n",
    "            \n",
    "            # Create query to get min and max values\n",
    "            min_max_query = f\"SELECT MIN({partition_column}) as min_val, MAX({partition_column}) as max_val FROM {source_schema}.{source_table} {self.table_data.get('JOIN_EXPR', '')} WHERE {partition_column} <> 0\"\n",
    "            if self.table_data.get('FILTER_EXP', '').strip() != '':\n",
    "                min_max_query = f\"{min_max_query} AND {self.table_data['FILTER_EXP']}\"\n",
    "            self.logger.info(f\"Executing min/max query: {min_max_query}\")\n",
    "            \n",
    "            # Execute query\n",
    "            df_min_max = self.execute_db_query(min_max_query)\n",
    "            \n",
    "            # Get min and max values from the DataFrame and convert to integers\n",
    "            min_raw = df_min_max['min_val'].iloc[0]\n",
    "            max_raw = df_min_max['max_val'].iloc[0]\n",
    "\n",
    "            min_val = int(min_raw) if min_raw is not None else None\n",
    "            max_val = int(max_raw) if max_raw is not None else None\n",
    "            \n",
    "            return min_val, max_val\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error getting min and max values: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_partitioned_query(self, partition_column, min_val, increment, partition_index, num_partitions):\n",
    "        \"\"\"Generate partitioned query based on min/max range using integer values\"\"\"\n",
    "        # Calculate start and end values as integers\n",
    "        start_value = int(min_val + (increment * partition_index))\n",
    "        \n",
    "        # For the last partition, use max_val + 1 to ensure we include the max value\n",
    "        if partition_index == num_partitions - 1:\n",
    "            end_value = int(min_val + (increment * (partition_index + 1))) + 1\n",
    "        else:\n",
    "            end_value = int(min_val + (increment * (partition_index + 1)))\n",
    "        \n",
    "        # Get columns and build the query\n",
    "        columns_aux = self.table_data['COLUMNS']\n",
    "        if self.table_data.get('ID_COLUMN', '') != '':\n",
    "            columns_aux = f\"{self.table_data['ID_COLUMN']} as id,\" + self.table_data['COLUMNS']\n",
    "        \n",
    "        # Apply column processing to handle SQL Server identifier length limits\n",
    "        columns_aux = self._apply_column_processing_to_query(columns_aux)\n",
    "        \n",
    "        # Use >= and < for the range to avoid overlaps\n",
    "        query = f\"SELECT {columns_aux} FROM {self.table_data.get('SOURCE_SCHEMA', '')}.{self.table_data.get('SOURCE_TABLE', '')} {self.table_data.get('JOIN_EXPR', '')} WHERE {partition_column} >= {start_value} AND {partition_column} < {end_value}\"\n",
    "        \n",
    "        # Add additional filters if they exist\n",
    "        if self.table_data.get('FILTER_EXP', '').strip() != '':\n",
    "            query += f\" AND ({self.table_data['FILTER_EXP']})\"\n",
    "            \n",
    "        self.logger.info(f\"Partitioned query {partition_index}: {query} (Range: {start_value} to {end_value})\")\n",
    "        return query\n",
    "    \n",
    "    def get_query_for_date_range(self, start, end):\n",
    "        \"\"\"Generate query for a date range\"\"\"\n",
    "        query = self.table_data['QUERY_BY_GLUE']\n",
    "        \n",
    "        if 'FILTER_TYPE' in self.table_data.keys():\n",
    "            start, end = self.change_date_format(start, end, self.table_data['FILTER_TYPE'])\n",
    "            self.logger.debug(f\"Start Date: {start}\")\n",
    "            self.logger.debug(f\"End Date: {end}\")\n",
    "\n",
    "        if ',' in self.table_data['FILTER_COLUMN']:\n",
    "            filter_columns = self.table_data['FILTER_COLUMN'].split(\",\")\n",
    "            first_filter = filter_columns[0]\n",
    "            last_filter = filter_columns[1]\n",
    "\n",
    "            query += f\" WHERE ({first_filter} IS NOT NULL and {first_filter} BETWEEN {start} AND {end}) OR ({last_filter} IS NOT NULL and {last_filter} BETWEEN {start} AND {end})\"\n",
    "        else:\n",
    "            first_filter = self.table_data['FILTER_COLUMN']\n",
    "            query += f\" WHERE {first_filter} is not null and {first_filter} BETWEEN {start} AND {end}\"\n",
    "            \n",
    "        self.logger.info(query)\n",
    "        return query\n",
    "    \n",
    "    def change_date_format(self, start, end, date_type):\n",
    "        \"\"\"Change date format based on database type\"\"\"\n",
    "        if date_type == 'smalldatetime':\n",
    "            date_format = f\"CONVERT(smalldatetime, 'date_to_replace', 120)\"\n",
    "\n",
    "        elif date_type == 'DATE':\n",
    "            date_format = f\"TO_DATE('date_to_replace', 'YYYY-MM-DD HH24:MI:SS')\"\n",
    "            end = end[:19]\n",
    "            start = start[:19]\n",
    "\n",
    "        elif date_type == 'TIMESTAMP(6)':\n",
    "            date_format = f\"TO_TIMESTAMP('date_to_replace', 'YYYY-MM-DD HH24:MI:SS.FF')\"\n",
    "\n",
    "        elif date_type == 'SQL_DATETIME':\n",
    "            date_format = f\"CONVERT(DATETIME, 'date_to_replace',  102)\"\n",
    "\n",
    "        elif date_type == 'BIGINT':\n",
    "            end = dt.datetime.strptime(end, \"%Y-%m-%d %H:%M:%S\")\n",
    "            end = int(end.timestamp())\n",
    "            start = dt.datetime.strptime(start, \"%Y-%m-%d %H:%M:%S\")\n",
    "            start = int(start.timestamp())\n",
    "            date_format = \"date_to_replace\"\n",
    "\n",
    "        end = date_format.replace(\"date_to_replace\", str(end))\n",
    "        start = date_format.replace(\"date_to_replace\", str(start))\n",
    "        return start, end\n",
    "    \n",
    "    def create_standard_query(self):\n",
    "        \"\"\"Create a standard query for non-incremental loads\"\"\"\n",
    "        columns_aux = self.table_data.get('COLUMNS', '*')\n",
    "        \n",
    "        if self.table_data.get('ID_COLUMN', '') != '':\n",
    "            columns_aux = f\"{self.table_data['ID_COLUMN']} as id,\" + self.table_data.get('COLUMNS', '*')\n",
    "        \n",
    "        # Apply column processing to handle SQL Server identifier length limits\n",
    "        processed_columns = self._apply_column_processing_to_query(columns_aux)\n",
    "        \n",
    "        query = f\"select {processed_columns} from {self.table_data.get('SOURCE_SCHEMA', 'CAN NOT FIND SCHEMA NAME')}.{self.table_data.get('SOURCE_TABLE', 'CAN NOT FIND TABLE NAME')} {self.table_data.get('JOIN_EXPR', '')} \"\n",
    "        \n",
    "        print(f\"=== COMPLETE QUERY ===\")\n",
    "        print(query)\n",
    "        print(f\"=== END COMPLETE QUERY ===\")\n",
    "        \n",
    "        if self.table_data.get('FILTER_EXP', '').strip() != '' or self.table_data.get('FILTER_COLUMN', '').strip() != '':\n",
    "            if self.table_data.get('LOAD_TYPE', 'full') == 'full':\n",
    "                FILTER_COLUMN = '0=0'\n",
    "            else:\n",
    "                lower_limit, upper_limit = self.get_limits_for_filter(\n",
    "                    self.table_data.get('DELAY_INCREMENTAL_INI', -2), \n",
    "                    self.table_data.get('FILTER_DATA_TYPE', \"\"))\n",
    "                FILTER_COLUMN = self.table_data.get('FILTER_COLUMN', '1=1').replace('{0}', lower_limit).replace('{1}', upper_limit)\n",
    "                \n",
    "            if self.table_data.get('FILTER_EXP', '').strip() != '':\n",
    "                FILTER_EXP = self.table_data['FILTER_EXP']\n",
    "            else:\n",
    "                FILTER_EXP = '0=0'\n",
    "                \n",
    "            query += f'where {FILTER_EXP} AND {FILTER_COLUMN}'\n",
    "            \n",
    "        print(f\"=== FINAL COMPLETE QUERY WITH FILTERS ===\")\n",
    "        print(query)\n",
    "        print(f\"=== END FINAL COMPLETE QUERY ===\")\n",
    "        return query\n",
    "    \n",
    "    def determine_load_strategy(self):\n",
    "        \"\"\"Determine the load strategy based on table configuration\"\"\"\n",
    "        number_threads = 1\n",
    "        incremental_load = False\n",
    "\n",
    "        load_type = self.table_data.get('LOAD_TYPE', '').strip().lower()\n",
    "        table_type = self.table_data.get('SOURCE_TABLE_TYPE', '')\n",
    "        partition_column = self.table_data.get('PARTITION_COLUMN', '').strip()\n",
    "\n",
    "        # Full load with partitioning\n",
    "        if load_type == 'full' and table_type == 't' and partition_column:\n",
    "            self.logger.info(\"Full load with partitioning based on min/max range\")\n",
    "\n",
    "            try:\n",
    "                min_val, max_val = self.get_min_max_values(partition_column)\n",
    "\n",
    "                if min_val is None or max_val is None:\n",
    "                    self.logger.warning(\"MIN o MAX es None, cambiando a carga estándar.\")\n",
    "                    raise ValueError(\"No min/max\")\n",
    "\n",
    "                range_size = max_val - min_val\n",
    "                number_threads = 30\n",
    "\n",
    "                if range_size < number_threads:\n",
    "                    number_threads = max(1, range_size)\n",
    "                    self.logger.info(f\"Reduciendo número de particiones a {number_threads} (rango: {range_size})\")\n",
    "\n",
    "                increment = max(1, range_size // number_threads)\n",
    "\n",
    "                self.logger.info(\n",
    "                    f\"Partition column: {partition_column}, Min: {min_val}, Max: {max_val}, \"\n",
    "                    f\"Range: {range_size}, Increment: {increment}, Partitions: {number_threads}\"\n",
    "                )\n",
    "\n",
    "                return {\n",
    "                    'load_type': 'partitioned_full',\n",
    "                    'number_threads': number_threads,\n",
    "                    'incremental_load': True,\n",
    "                    'partition_column': partition_column,\n",
    "                    'min_val': min_val,\n",
    "                    'max_val': max_val,\n",
    "                    'increment': increment\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"No se pudo determinar min/max. Usando estrategia estándar. Motivo: {e}\")\n",
    "\n",
    "        # Incremental between-date load\n",
    "        if load_type == 'between-date':\n",
    "            self.logger.info(\"Incremental load with date range\")\n",
    "\n",
    "            start = self.table_data.get('START_VALUE', '').strip()\n",
    "            end = self.table_data.get('END_VALUE', '').strip()\n",
    "\n",
    "            if not start or not end:\n",
    "                self.logger.warning(\"START_VALUE o END_VALUE no definidos. Usando estrategia estándar.\")\n",
    "                return {\n",
    "                    'load_type': 'standard',\n",
    "                    'number_threads': number_threads,\n",
    "                    'incremental_load': incremental_load\n",
    "                }\n",
    "\n",
    "            try:\n",
    "                number_threads = int(self.config.get('THREADS_FOR_INCREMENTAL_LOADS', 1))\n",
    "            except ValueError:\n",
    "                number_threads = 1\n",
    "                self.logger.warning(\"THREADS_FOR_INCREMENTAL_LOADS inválido, usando 1 hilo.\")\n",
    "\n",
    "            start_dt = self.transform_to_dt(start)\n",
    "            end_dt = self.transform_to_dt(end)\n",
    "            delta = (end_dt - start_dt) / number_threads\n",
    "\n",
    "            return {\n",
    "                'load_type': 'between_date',\n",
    "                'number_threads': number_threads,\n",
    "                'incremental_load': True,\n",
    "                'start_dt': start_dt,\n",
    "                'end_dt': end_dt,\n",
    "                'delta': delta\n",
    "            }\n",
    "\n",
    "        # Default case\n",
    "        self.logger.info(\"Usando carga estándar\")\n",
    "        return {\n",
    "            'load_type': 'standard',\n",
    "            'number_threads': number_threads,\n",
    "            'incremental_load': incremental_load\n",
    "        }\n",
    "    \n",
    "    def extract_data(self):\n",
    "        \"\"\"Main method to extract data from source and load to S3 with controlled concurrency\"\"\"\n",
    "        try:\n",
    "            # Delete existing data in the target S3 path\n",
    "            self.delete_from_target(self.bucket, self.day_route)\n",
    "            \n",
    "            # Determine load strategy\n",
    "            load_strategy = self.determine_load_strategy()\n",
    "            total_tasks = load_strategy['number_threads']\n",
    "            incremental_load = load_strategy['incremental_load']\n",
    "            \n",
    "            # Set maximum concurrent workers (6 as requested)\n",
    "            max_concurrent_workers = min(6, total_tasks)\n",
    "            self.logger.info(f\"Processing {total_tasks} tasks with maximum {max_concurrent_workers} concurrent workers\")\n",
    "            \n",
    "            # Generate all queries upfront based on load strategy\n",
    "            all_queries = []\n",
    "            for i in range(total_tasks):\n",
    "                if incremental_load:\n",
    "                    if load_strategy['load_type'] == 'partitioned_full':\n",
    "                        # Generate partitioned query for full load with partitioning\n",
    "                        query = self.get_partitioned_query(\n",
    "                            load_strategy['partition_column'],\n",
    "                            load_strategy['min_val'],\n",
    "                            load_strategy['increment'],\n",
    "                            i,\n",
    "                            total_tasks\n",
    "                        )\n",
    "                    else:\n",
    "                        # Generate query for date-based incremental load\n",
    "                        start_dt = load_strategy['start_dt']\n",
    "                        delta = load_strategy['delta']\n",
    "                        start_str = str(start_dt + delta * i)[:19]\n",
    "                        end_str = str(start_dt + delta * (i + 1))[:19]\n",
    "                        query = self.get_query_for_date_range(start_str, end_str)\n",
    "                else:\n",
    "                    # Generate standard query for non-incremental loads\n",
    "                    query = self.create_standard_query()\n",
    "                \n",
    "                # Final query print for debugging\n",
    "                print(f\"Final Query {i}: {query}\")\n",
    "                \n",
    "                all_queries.append(query)\n",
    "            \n",
    "            # Process tasks in batches with controlled concurrency\n",
    "            completed_tasks = 0\n",
    "            active_futures = set()\n",
    "            \n",
    "            with futures.ThreadPoolExecutor(max_workers=max_concurrent_workers) as executor:\n",
    "                # Initial batch of tasks\n",
    "                for i in range(min(max_concurrent_workers, total_tasks)):\n",
    "                    future = executor.submit(\n",
    "                        self.get_data,\n",
    "                        all_queries[i],\n",
    "                        self.s3_raw_path,\n",
    "                        i,\n",
    "                        total_tasks\n",
    "                    )\n",
    "                    active_futures.add(future)\n",
    "                    \n",
    "                # Process tasks as they complete\n",
    "                next_task_idx = max_concurrent_workers\n",
    "                \n",
    "                while active_futures and completed_tasks < total_tasks:\n",
    "                    # Wait for any task to complete\n",
    "                    done, active_futures = futures.wait(\n",
    "                        active_futures, \n",
    "                        return_when=futures.FIRST_COMPLETED\n",
    "                    )\n",
    "                    \n",
    "                    # Process completed tasks\n",
    "                    for future in done:\n",
    "                        try:\n",
    "                            future.result()  # Check for exceptions\n",
    "                            completed_tasks += 1\n",
    "                            self.logger.info(f\"Completed task {completed_tasks}/{total_tasks}\")\n",
    "                        except Exception as e:\n",
    "                            self.logger.error(f\"Task failed with error: {str(e)}\")\n",
    "                            raise\n",
    "                    \n",
    "                    # Queue up new tasks if available\n",
    "                    while len(active_futures) < max_concurrent_workers and next_task_idx < total_tasks:\n",
    "                        future = executor.submit(\n",
    "                            self.get_data,\n",
    "                            all_queries[next_task_idx],\n",
    "                            self.s3_raw_path,\n",
    "                            next_task_idx,\n",
    "                            total_tasks\n",
    "                        )\n",
    "                        active_futures.add(future)\n",
    "                        self.logger.info(f\"Started task {next_task_idx + 1}/{total_tasks}\")\n",
    "                        next_task_idx += 1\n",
    "            \n",
    "            self.logger.info(f\"All {total_tasks} tasks completed successfully\")\n",
    "            \n",
    "            # Log success\n",
    "            self._log_success()\n",
    "            \n",
    "            return True\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            self.logger.error(f\"Error in extract_data: {error_msg}\")\n",
    "            self._log_error(error_msg)\n",
    "            raise Exception(f\"Failed to extract data: {error_msg}\")\n",
    "\n",
    "config = {}\n",
    "if IS_AWS_GLUE:\n",
    "    from awsglue.utils import getResolvedOptions    \n",
    "    args = getResolvedOptions(\n",
    "        sys.argv, ['S3_RAW_PREFIX', 'ARN_TOPIC_SUCCESS', 'PROJECT_NAME', 'TEAM', 'DATA_SOURCE', 'ENVIRONMENT', 'REGION', 'DYNAMO_LOGS_TABLE', 'ARN_TOPIC_FAILED', 'TABLE_NAME', 'TABLES_CSV_S3', 'CREDENTIALS_CSV_S3', 'COLUMNS_CSV_S3', 'ENDPOINT_NAME'])\n",
    "\n",
    "    config = {\n",
    "        \"S3_RAW_PREFIX\": args[\"S3_RAW_PREFIX\"],\n",
    "        \"DYNAMO_LOGS_TABLE\": args[\"DYNAMO_LOGS_TABLE\"],\n",
    "        \"ENVIRONMENT\": args[\"ENVIRONMENT\"],\n",
    "        \"PROJECT_NAME\": args[\"PROJECT_NAME\"],\n",
    "        \"TEAM\": args[\"TEAM\"],\n",
    "        \"DATA_SOURCE\": args[\"DATA_SOURCE\"],\n",
    "        \"THREADS_FOR_INCREMENTAL_LOADS\": 6,\n",
    "        \"TOPIC_ARN\": args[\"ARN_TOPIC_FAILED\"], \n",
    "        \"REGION\": args[\"REGION\"],\n",
    "        \"TABLE_NAME\": args[\"TABLE_NAME\"],\n",
    "        \"TABLES_CSV_S3\": args[\"TABLES_CSV_S3\"],\n",
    "        \"CREDENTIALS_CSV_S3\": args[\"CREDENTIALS_CSV_S3\"],\n",
    "        \"COLUMNS_CSV_S3\": args[\"COLUMNS_CSV_S3\"],\n",
    "        \"ENDPOINT_NAME\": args[\"ENDPOINT_NAME\"]\n",
    "    }\n",
    "else: \n",
    "    config = {\n",
    "                'S3_RAW_PREFIX': \"s3://sofia-dev-datalake-510543735161-us-east-1-raw-s3/\",\n",
    "                'ARN_TOPIC_FAILED': \"arn:aws:sns:us-east-1:510543735161:sofia-dev-datalake-failed-sns\",\n",
    "                'ARN_TOPIC_SUCCESS': \"arn:aws:sns:us-east-1:510543735161:sofia-dev-datalake-success-sns\",\n",
    "                'PROJECT_NAME': \"datalake\",\n",
    "                'TEAM': \"apdayc\",\n",
    "                'DATA_SOURCE': \"bigmagic\",\n",
    "                'ENVIRONMENT': \"DEV\",\n",
    "                'REGION': \"us-east-1\",\n",
    "                'DYNAMO_LOGS_TABLE': \"sofia-dev-datalake-logs-ddb\",\n",
    "                'ENDPOINT_NAME': \"PEBDDATA2\",\n",
    "                'TABLE_NAME': \"M_PAIS\",\n",
    "                # Configuration parameters - pass CSV paths instead of large JSON to avoid template size limits\n",
    "                'TABLES_CSV_S3': \"../../artifacts/configuration/csv/tables.csv\",\n",
    "                'CREDENTIALS_CSV_S3': \"../../artifacts/configuration/csv/credentials.csv\",\n",
    "                'COLUMNS_CSV_S3': \"../../artifacts/configuration/csv/columns.csv\",\n",
    "            }\n",
    "     \n",
    "region_name = config[\"REGION\"]\n",
    "boto3.setup_default_session(profile_name='prod-compliance-admin', region_name=region_name)\n",
    "\n",
    "logger = custom_logger(__name__)\n",
    "\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "logger.info(\"Version: SQL Server Identifier Parsing Fix v2.0\")\n",
    "logger.info(f\"Table: {config['TABLE_NAME']}\")\n",
    "logger.info(f\"EndPoint: {config['ENDPOINT_NAME']}\")\n",
    "logger.info(\"=\" * 80)\n",
    "\n",
    "logger.info(\"Starting data extraction process\")\n",
    "logger.info(f\"Configuration: {config}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "710851db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"level\":\"INFO\",\"location\":\"<module>:3\",\"message\":\"Creating extractor instance\",\"timestamp\":\"2025-08-01 02:19:57,453+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"__init__:37\",\"message\":\"Configured helper for DynamoDB table: sofia-dev-datalake-logs-ddb\",\"timestamp\":\"2025-08-01 02:19:58,199+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"__init__:34\",\"message\":\"Configured helper for S3 bucket: sofia-dev-datalake-510543735161-us-east-1-raw-s3\",\"timestamp\":\"2025-08-01 02:19:58,432+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"_load_csv_configurations:131\",\"message\":\"TABLES_CSV_S3: ../../artifacts/configuration/csv/tables.csv\",\"timestamp\":\"2025-08-01 02:19:58,433+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"_load_csv_configurations:133\",\"message\":\"tables_data: {'COLUMNS': \\\"dbo.func_cas_todatetime(fecultmod,horultimod) lastmodifydate, compania, califica3,replace(replace(descrip, char(13),''), char(10),'') descrip, estado, feccrea, horcrea, usucrea, fecultmod, horultimod, ultusumod, califica3_corp\\\", 'DELAY_INCREMENTAL_INI': '', 'FILTER_COLUMN': '', 'FILTER_DATA_TYPE': '', 'FILTER_EXP': \\\"compania in (select compania from dbo.mcompa1f b where b.flgbi = 'a')\\\", 'ID_COLUMN': \\\"rtrim(ltrim(compania)) + '|' + rtrim(ltrim(califica3))\\\", 'JOIN_EXPR': '', 'PROCESS_ID': '10', 'SOURCE_SCHEMA': 'dbo', 'SOURCE_TABLE': 'bartic3f', 'SOURCE_TABLE_TYPE': 'm', 'STAGE_TABLE_NAME': 'm_formato', 'STATUS': 'a'}\",\"timestamp\":\"2025-08-01 02:19:58,437+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"_load_csv_configurations:134\",\"message\":\"CREDENTIALS_CSV_S3: ../../artifacts/configuration/csv/credentials.csv\",\"timestamp\":\"2025-08-01 02:19:58,438+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"_load_csv_configurations:136\",\"message\":\"credentials_data: {'ENDPOINT_NAME': 'PEBDDATA2', 'BD_TYPE': 'mssql', 'DB_PORT_NUMBER': '1433', 'IS_PRINCIPAL': 'true', 'SRC_DB_NAME': 'BDDATA', 'COUNTRIES': 'PE', 'SRC_DB_SECRET': 'password', 'SRC_DB_USERNAME': 'usr_datalake', 'SRC_SERVER_NAME': '172.16.0.33', 'INSTANCE': 'PE', 'ENV': 'DEV'}\",\"timestamp\":\"2025-08-01 02:19:58,439+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"_load_csv_configurations:137\",\"message\":\"COLUMNS_CSV_S3: ../../artifacts/configuration/csv/columns.csv\",\"timestamp\":\"2025-08-01 02:19:58,440+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"_load_csv_configurations:139\",\"message\":\"columns_data: {'COLUMN_NAME': 'cod_compania', 'COLUMN_ID': '2', 'IS_FILTER_DATE': 'F', 'IS_ID': 'F', 'IS_ORDER_BY': 'F', 'IS_PARTITION': 'F', 'NEW_DATA_TYPE': 'string', 'TABLE_NAME': 'M_FORMATO', 'TRANSFORMATION': 'fn_transform_ClearString(compania)'}\",\"timestamp\":\"2025-08-01 02:19:58,458+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"_load_csv_configurations:141\",\"message\":\"config: {'S3_RAW_PREFIX': 's3://sofia-dev-datalake-510543735161-us-east-1-raw-s3/', 'ARN_TOPIC_FAILED': 'arn:aws:sns:us-east-1:510543735161:sofia-dev-datalake-failed-sns', 'ARN_TOPIC_SUCCESS': 'arn:aws:sns:us-east-1:510543735161:sofia-dev-datalake-success-sns', 'PROJECT_NAME': 'datalake', 'TEAM': 'apdayc', 'DATA_SOURCE': 'bigmagic', 'ENVIRONMENT': 'DEV', 'REGION': 'us-east-1', 'DYNAMO_LOGS_TABLE': 'sofia-dev-datalake-logs-ddb', 'ENDPOINT_NAME': 'PEBDDATA2', 'TABLE_NAME': 'M_PAIS', 'TABLES_CSV_S3': '../../artifacts/configuration/csv/tables.csv', 'CREDENTIALS_CSV_S3': '../../artifacts/configuration/csv/credentials.csv', 'COLUMNS_CSV_S3': '../../artifacts/configuration/csv/columns.csv'}\",\"timestamp\":\"2025-08-01 02:19:58,460+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"_load_csv_configurations:153\",\"message\":\"Found table configuration for M_PAIS\",\"timestamp\":\"2025-08-01 02:19:58,461+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"_process_columns_field:671\",\"message\":\"Processing COLUMNS field for table: m_pais\",\"timestamp\":\"2025-08-01 02:19:58,462+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"_process_columns_field:708\",\"message\":\"Processed 13 columns successfully\",\"timestamp\":\"2025-08-01 02:19:58,464+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"_load_csv_configurations:187\",\"message\":\"Determined LOAD_TYPE: full for table M_PAIS\",\"timestamp\":\"2025-08-01 02:19:58,465+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"_load_csv_configurations:195\",\"message\":\"Loaded configuration for table: M_PAIS\",\"timestamp\":\"2025-08-01 02:19:58,467+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"_load_csv_configurations:196\",\"message\":\"Columns count: 12\",\"timestamp\":\"2025-08-01 02:19:58,468+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"_get_clean_table_name:308\",\"message\":\"Clean table name extracted: 'bubige1f' -> 'bubige1f'\",\"timestamp\":\"2025-08-01 02:19:58,470+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"__init__:23\",\"message\":\"Inicializando SecretsHelper para el secreto: dev/datalake/apdayc/bigmagic\",\"timestamp\":\"2025-08-01 02:19:58,493+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"get_secret_value:31\",\"message\":\"Intentando obtener el secreto: dev/datalake/apdayc/bigmagic\",\"timestamp\":\"2025-08-01 02:19:58,494+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"get_secret_value:35\",\"message\":\"Secreto obtenido con éxito: dev/datalake/apdayc/bigmagic\",\"timestamp\":\"2025-08-01 02:19:58,597+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"get_secret_value:43\",\"message\":\"Retornando valor para la clave específica: password\",\"timestamp\":\"2025-08-01 02:19:58,598+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"create_helper:34\",\"message\":\"Creating database helper for mssql database: BDDATA on 172.16.0.33\",\"timestamp\":\"2025-08-01 02:19:58,598+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"__init__:33\",\"message\":\"Configured base helper for database: BDDATA on 172.16.0.33\",\"timestamp\":\"2025-08-01 02:19:58,599+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"__init__:42\",\"message\":\"Configured helper for SQL Server database: BDDATA on 172.16.0.33\",\"timestamp\":\"2025-08-01 02:19:58,600+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"init_db_connection:282\",\"message\":\"server: 172.16.0.33\",\"timestamp\":\"2025-08-01 02:19:58,601+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"init_db_connection:283\",\"message\":\"port: 1433\",\"timestamp\":\"2025-08-01 02:19:58,604+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"init_db_connection:284\",\"message\":\"db_name: BDDATA\",\"timestamp\":\"2025-08-01 02:19:58,606+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"init_db_connection:285\",\"message\":\"username: usr_datalake\",\"timestamp\":\"2025-08-01 02:19:58,608+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"init_db_connection:286\",\"message\":\"password: usr_datalake\",\"timestamp\":\"2025-08-01 02:19:58,608+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"init_db_connection:287\",\"message\":\"Database connection initialized for mssql database\",\"timestamp\":\"2025-08-01 02:19:58,610+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"init_db_connection:288\",\"message\":\"driver: com.microsoft.sqlserver.jdbc.SQLServerDriver\",\"timestamp\":\"2025-08-01 02:19:58,610+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"init_db_connection:289\",\"message\":\"url: 172.16.0.33:1433\",\"timestamp\":\"2025-08-01 02:19:58,611+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"<module>:5\",\"message\":\"Extractor instance created\",\"timestamp\":\"2025-08-01 02:19:58,621+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"list_objects:495\",\"message\":\"Listing objects in bucket sofia-dev-datalake-510543735161-us-east-1-raw-s3\",\"timestamp\":\"2025-08-01 02:19:58,622+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"list_objects:526\",\"message\":\"Found 0 objects across 1 pages\",\"timestamp\":\"2025-08-01 02:19:58,738+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"delete_from_target:378\",\"message\":\"No objects found to delete in apdayc/bigmagic/bubige1f/year=2025/month=07/day=31/\",\"timestamp\":\"2025-08-01 02:19:58,740+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"determine_load_strategy:1069\",\"message\":\"Usando carga estándar\",\"timestamp\":\"2025-08-01 02:19:58,740+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"extract_data:1089\",\"message\":\"Processing 1 tasks with maximum 1 concurrent workers\",\"timestamp\":\"2025-08-01 02:19:58,741+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "=== COMPLETE QUERY ===\n",
      "select rtrim(ltrim(codletr)) as id, dbo.func_cas_todatetime(fecultmod,horultimod) lastmodifydate, rtrim(ltrim(pais)) pais, replace(replace(descrip, char(13),''), char(10),'') descrip, continente, codletr, estado, feccrea, horcrea, usucrea, fecultmod, horultimod, ultusumod, cast(cast(zonareg as int) as varchar(20)) zonareg from dbo.bubige1f  \n",
      "=== END COMPLETE QUERY ===\n",
      "=== FINAL COMPLETE QUERY WITH FILTERS ===\n",
      "select rtrim(ltrim(codletr)) as id, dbo.func_cas_todatetime(fecultmod,horultimod) lastmodifydate, rtrim(ltrim(pais)) pais, replace(replace(descrip, char(13),''), char(10),'') descrip, continente, codletr, estado, feccrea, horcrea, usucrea, fecultmod, horultimod, ultusumod, cast(cast(zonareg as int) as varchar(20)) zonareg from dbo.bubige1f  where pais in (select pais from dbo.mcompa1f where flgbi = 'a') AND 0=0\n",
      "=== END FINAL COMPLETE QUERY ===\n",
      "Final Query 0: select rtrim(ltrim(codletr)) as id, dbo.func_cas_todatetime(fecultmod,horultimod) lastmodifydate, rtrim(ltrim(pais)) pais, replace(replace(descrip, char(13),''), char(10),'') descrip, continente, codletr, estado, feccrea, horcrea, usucrea, fecultmod, horultimod, ultusumod, cast(cast(zonareg as int) as varchar(20)) zonareg from dbo.bubige1f  where pais in (select pais from dbo.mcompa1f where flgbi = 'a') AND 0=0\n",
      "{\"level\":\"INFO\",\"location\":\"get_data:795\",\"message\":\"select rtrim(ltrim(codletr)) as id, dbo.func_cas_todatetime(fecultmod,horultimod) lastmodifydate, rtrim(ltrim(pais)) pais, replace(replace(descrip, char(13),''), char(10),'') descrip, continente, codletr, estado, feccrea, horcrea, usucrea, fecultmod, horultimod, ultusumod, cast(cast(zonareg as int) as varchar(20)) zonareg from dbo.bubige1f  where pais in (select pais from dbo.mcompa1f where flgbi = 'a') AND 0=0\",\"timestamp\":\"2025-08-01 02:19:58,744+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"execute_db_query:425\",\"message\":\"Executing query on database: {query}\",\"timestamp\":\"2025-08-01 02:19:58,746+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"execute_query_as_dict:105\",\"message\":\"Query executed successfully, returned 1 rows as dictionaries\",\"timestamp\":\"2025-08-01 02:19:58,871+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"write_dataframe_to_s3_parquet:653\",\"message\":\"Successfully wrote DataFrame to s3://sofia-dev-datalake-510543735161-us-east-1-raw-s3/apdayc/bigmagic/bubige1f/year=2025/month=07/day=31/data_thread_0_ca2f8769.parquet as Parquet\",\"timestamp\":\"2025-08-01 02:19:58,995+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"get_data:837\",\"message\":\"finished thread n: 0\",\"timestamp\":\"2025-08-01 02:19:58,998+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"get_data:838\",\"message\":\"Data sample:    id       lastmodifydate pais                    descrip       continente  \\\\\\n0  PE  2022-07-22 12:00:01   01  PERU                       America del Sur   \\n\\n  codletr estado  feccrea horcrea     usucrea  fecultmod horultimod  \\\\\\n0      PE      A   736192  000000  SYSTEM         738358     000001   \\n\\n    ultusumod zonareg  \\n0  SYSTEM        None  \",\"timestamp\":\"2025-08-01 02:19:59,003+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"extract_data:1151\",\"message\":\"Completed task 1/1\",\"timestamp\":\"2025-08-01 02:19:59,006+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"extract_data:1169\",\"message\":\"All 1 tasks completed successfully\",\"timestamp\":\"2025-08-01 02:19:59,007+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"_get_clean_table_name:308\",\"message\":\"Clean table name extracted: 'bubige1f' -> 'bubige1f'\",\"timestamp\":\"2025-08-01 02:19:59,008+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"_log_success:342\",\"message\":\"process_id: DLB_M_bubige1f_20250731_211933\",\"timestamp\":\"2025-08-01 02:19:59,011+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"put_item:132\",\"message\":\"Inserting item into table sofia-dev-datalake-logs-ddb\",\"timestamp\":\"2025-08-01 02:19:59,012+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"put_item:139\",\"message\":\"Item inserted successfully\",\"timestamp\":\"2025-08-01 02:19:59,076+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"_log_success:355\",\"message\":\"DynamoDB updated with success status\",\"timestamp\":\"2025-08-01 02:19:59,077+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"<module>:8\",\"message\":\"Extraction completed\",\"timestamp\":\"2025-08-01 02:19:59,079+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n",
      "{\"level\":\"INFO\",\"location\":\"<module>:10\",\"message\":\"Successfully extracted data for table M_PAIS\",\"timestamp\":\"2025-08-01 02:19:59,080+0000\",\"service\":\"service_undefined\",\"name\":\"aje_libs.common.helpers.dynamodb_helper\"}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Create extractor instance\n",
    "    logger.info(\"Creating extractor instance\")\n",
    "    extractor = DataExtractor(config)\n",
    "    logger.info(\"Extractor instance created\")\n",
    "    # Run extraction\n",
    "    success = extractor.extract_data()\n",
    "    logger.info(\"Extraction completed\")\n",
    "    if success:\n",
    "        logger.info(f\"Successfully extracted data for table {config['TABLE_NAME']}\") \n",
    "    else:\n",
    "        logger.error(f\"Failed to extract data for table {config['TABLE_NAME']}\") \n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred during extraction: {str(e)}\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_datalake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
