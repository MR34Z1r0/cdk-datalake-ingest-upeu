{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c54c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import pytz\n",
    "#from awsglue.utils import getResolvedOptions\n",
    "\n",
    "# Obtener parámetros del trabajo\n",
    "#args = getResolvedOptions(\n",
    "#    sys.argv, ['JOB_NAME', 'S3_STAGE_PREFIX', 'DYNAMO_CONFIG_TABLE', 'DYNAMO_ENDPOINT_TABLE', \n",
    "#                'ENDPOINT', 'PROCESS_ID', 'ARN_ROLE_CRAWLER', 'PROJECT_NAME', 'TEAM', 'DATA_SOURCE'])\n",
    "##################################\n",
    "boto3.setup_default_session(profile_name='prd-valorx-admin', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb62d48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 14:32:44,384 sofia-dev-datalake-crawler_stage-job DEBUG Rol ARN obtenido: arn:aws:iam::566121885938:role/CdkDatalakeIngestBigMagic-sofiadevdatalakecrawlerst-Av8iwB3ELbnA\n"
     ]
    }
   ],
   "source": [
    "glue_crawler_manager = GlueCrawlerManager(logger)\n",
    "job_role_arn = glue_crawler_manager.get_job_role_arn('sofia-dev-datalake-crawler_stage-job')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87a8d33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 14:33:17,561 sofia-dev-datalake-crawler_stage-job ERROR Error al conceder permisos LF-Tag: An error occurred (AccessDeniedException) when calling the GrantPermissions operation: Resource does not exist or requester is not authorized to access requested permissions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glue_crawler_manager.grant_lf_tag_permissions(job_role_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f691047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 15:52:41,066 botocore.credentials INFO Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-05-18 15:52:42,152 root INFO Procesando endpoint: PEBDDATA\n",
      "2025-05-18 15:52:42,153 root INFO Base de datos: sofia_bigmagic_pebddata_stage\n",
      "2025-05-18 15:52:42,154 root INFO Crawler: sofia_bigmagic_pebddata_stage_crawler\n",
      "2025-05-18 15:52:43,035 root INFO Total de tablas: 106, Tablas sin crawler: 0\n",
      "2025-05-18 15:52:44,281 root INFO Crawler sofia_bigmagic_pebddata_stage_crawler no encontrado\n",
      "2025-05-18 15:52:44,282 root INFO Verificando existencia de la base de datos en el catálogo\n",
      "2025-05-18 15:52:44,516 root INFO Base de datos sofia_bigmagic_pebddata_stage encontrada\n",
      "2025-05-18 15:52:44,518 root INFO Creando nuevo crawler para base de datos existente sofia_bigmagic_pebddata_stage\n",
      "2025-05-18 15:52:44,998 root INFO Crawler sofia_bigmagic_pebddata_stage_crawler creado exitosamente\n",
      "2025-05-18 15:52:56,842 root INFO Crawler sofia_bigmagic_pebddata_stage_crawler iniciado exitosamente\n",
      "2025-05-18 15:52:56,843 root INFO Proceso completado exitosamente\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import pytz\n",
    "#from awsglue.utils import getResolvedOptions\n",
    "\n",
    "# Obtener parámetros del trabajo\n",
    "#args = getResolvedOptions(\n",
    "#    sys.argv, ['JOB_NAME', 'S3_STAGE_PREFIX', 'DYNAMO_CONFIG_TABLE', 'DYNAMO_ENDPOINT_TABLE', \n",
    "#                'ENDPOINT', 'PROCESS_ID', 'ARN_ROLE_CRAWLER', 'PROJECT_NAME', 'TEAM', 'DATA_SOURCE'])\n",
    "##################################\n",
    "boto3.setup_default_session(profile_name='prd-valorx-admin', region_name='us-east-1')\n",
    "##################################\n",
    "args = {\n",
    "    'JOB_NAME': 'sofia-dev-datalake-crawler_stage-job',\n",
    "    'S3_STAGE_PREFIX': 's3://sofia-566121885938-us-east-1-dev-datalake-stage-s3/',\n",
    "    'DYNAMO_CONFIG_TABLE': 'sofia-dev-datalake-configuration-ddb',\n",
    "    'DYNAMO_ENDPOINT_TABLE': 'sofia-dev-datalake-credentials-ddb',\n",
    "    'ENDPOINT': 'PEBDDATA',\n",
    "    'PROCESS_ID': '10',\n",
    "    'ARN_ROLE_CRAWLER': 'arn:aws:iam::566121885938:role/sofia-dev-datalake-crawler_stage-role',\n",
    "    'PROJECT_NAME': 'datalake',\n",
    "    'TEAM': 'sofia',\n",
    "    'DATA_SOURCE': 'bigmagic'    \n",
    "}\n",
    "# Obtener parámetros del trabajo\n",
    "#args = getResolvedOptions(\n",
    "#    sys.argv, ['JOB_NAME', 'S3_STAGE_PREFIX', 'DYNAMO_CONFIG_TABLE', 'DYNAMO_ENDPOINT_TABLE', \n",
    "#                'ENDPOINT', 'PROCESS_ID', 'ARN_ROLE_CRAWLER', 'PROJECT_NAME', 'TEAM', 'DATA_SOURCE'])\n",
    "\n",
    "# Configuración del logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Clase utilitaria para gestionar recursos de AWS Glue\n",
    "class GlueCrawlerManager:\n",
    "    \"\"\"Clase utilitaria para gestionar recursos de AWS Glue\"\"\"\n",
    "    \n",
    "    def __init__(self, logger=None):\n",
    "        \"\"\"Inicializar la clase con servicios AWS\"\"\"\n",
    "        self.logger = logger or logging.getLogger(__name__)\n",
    "        \n",
    "        try:\n",
    "            self.dynamodb = boto3.resource('dynamodb')\n",
    "            self.client_glue = boto3.client('glue')\n",
    "            self.client_lakeformation = boto3.client('lakeformation')\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al inicializar servicios AWS: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def create_database(self, database_name):\n",
    "        \"\"\"Crear base de datos en el catálogo de datos\"\"\"\n",
    "        try:\n",
    "            self.client_glue.create_database(\n",
    "                DatabaseInput={\n",
    "                    'Name': database_name\n",
    "                }\n",
    "            )\n",
    "            self.logger.info(f\"Base de datos {database_name} creada exitosamente\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al crear la base de datos {database_name}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def database_exists(self, database_name):\n",
    "        \"\"\"Verificar si existe la base de datos en el catálogo\"\"\"\n",
    "        try:\n",
    "            self.client_glue.get_database(\n",
    "                Name=database_name\n",
    "            )\n",
    "            self.logger.info(f\"Base de datos {database_name} encontrada\")\n",
    "            return True\n",
    "        except self.client_glue.exceptions.EntityNotFoundException:\n",
    "            self.logger.info(f\"Base de datos {database_name} no encontrada\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al verificar la base de datos {database_name}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def get_job_role_arn(self, job_name):\n",
    "        \"\"\"Obtener el ARN del rol del trabajo\"\"\"\n",
    "        try:\n",
    "            role = self.client_glue.get_job(JobName=job_name)['Job']['Role']\n",
    "            self.logger.info(f\"Rol ARN obtenido: {role}\")\n",
    "            return role\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al obtener el rol ARN del trabajo {job_name}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def grant_database_permissions(self, role_arn, database_name):\n",
    "        \"\"\"Asignar permisos de la base de datos en Lake Formation\"\"\"\n",
    "        try:\n",
    "            self.client_lakeformation.grant_permissions(\n",
    "                Principal={\n",
    "                    'DataLakePrincipalIdentifier': role_arn\n",
    "                },\n",
    "                Resource={\n",
    "                    'Database': {\n",
    "                        'Name': database_name\n",
    "                    },\n",
    "                },\n",
    "                Permissions=[\n",
    "                    'ALL',\n",
    "                ],\n",
    "                PermissionsWithGrantOption=[\n",
    "                    'ALL',\n",
    "                ]\n",
    "            )\n",
    "            self.logger.info(f\"Permisos concedidos a {role_arn} sobre {database_name}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al conceder permisos en Lake Formation: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def grant_lf_tag_permissions(self, role_arn, tag_key=\"Level\", tag_values=[\"Stage\"]):\n",
    "        \"\"\"Asignar permisos de etiquetas LF al rol\"\"\"\n",
    "        try:\n",
    "            self.client_lakeformation.grant_permissions(\n",
    "                Principal={\n",
    "                    'DataLakePrincipalIdentifier': role_arn\n",
    "                },\n",
    "                Resource={\n",
    "                    'LFTag': {\n",
    "                        'TagKey': tag_key,\n",
    "                        'TagValues': tag_values\n",
    "                    },\n",
    "                },\n",
    "                Permissions=[\n",
    "                    'ASSOCIATE',\n",
    "                ],\n",
    "                PermissionsWithGrantOption=[\n",
    "                    'ASSOCIATE',\n",
    "                ]\n",
    "            )\n",
    "            self.logger.info(f\"Permisos LF-Tag concedidos a {role_arn}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al conceder permisos LF-Tag: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def add_lf_tags_to_database(self, database_name, tag_key=\"Level\", tag_values=[\"Stage\"]):\n",
    "        \"\"\"Añadir etiquetas LF a la base de datos\"\"\"\n",
    "        try:\n",
    "            self.client_lakeformation.add_lf_tags_to_resource(\n",
    "                Resource={\n",
    "                    'Database': {\n",
    "                        'Name': database_name\n",
    "                    },\n",
    "                },\n",
    "                LFTags=[\n",
    "                    {\n",
    "                        'TagKey': tag_key,\n",
    "                        'TagValues': tag_values\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "            self.logger.info(f\"Etiquetas LF añadidas a {database_name}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al añadir etiquetas LF a la base de datos: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def create_delta_crawler(self, crawler_name, role_arn, database_name, delta_targets):\n",
    "        \"\"\"Crear crawler para objetivos Delta\"\"\"\n",
    "        try:\n",
    "            if not delta_targets:\n",
    "                self.logger.warning(\"No hay objetivos para crear el crawler\")\n",
    "                return False\n",
    "                \n",
    "            self.client_glue.create_crawler(\n",
    "                Name=crawler_name,\n",
    "                Role=role_arn,\n",
    "                DatabaseName=database_name,\n",
    "                Targets={\n",
    "                    'DeltaTargets': delta_targets\n",
    "                }\n",
    "            )\n",
    "            self.logger.info(f\"Crawler {crawler_name} creado exitosamente\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al crear crawler {crawler_name}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def update_delta_crawler(self, crawler_name, role_arn, database_name, delta_targets):\n",
    "        \"\"\"Actualizar crawler existente\"\"\"\n",
    "        try:\n",
    "            if not delta_targets:\n",
    "                self.logger.warning(\"No hay objetivos para actualizar el crawler\")\n",
    "                return False\n",
    "                \n",
    "            self.client_glue.update_crawler(\n",
    "                Name=crawler_name,\n",
    "                Role=role_arn,\n",
    "                DatabaseName=database_name,\n",
    "                Targets={\n",
    "                    'DeltaTargets': delta_targets\n",
    "                }\n",
    "            )\n",
    "            self.logger.info(f\"Crawler {crawler_name} actualizado exitosamente\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al actualizar crawler {crawler_name}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def crawler_exists(self, crawler_name):\n",
    "        \"\"\"Verificar si existe el crawler\"\"\"\n",
    "        try:\n",
    "            self.client_glue.get_crawler(\n",
    "                Name=crawler_name\n",
    "            )\n",
    "            self.logger.info(f\"Crawler {crawler_name} encontrado\")\n",
    "            return True\n",
    "        except self.client_glue.exceptions.EntityNotFoundException:\n",
    "            self.logger.info(f\"Crawler {crawler_name} no encontrado\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al verificar crawler {crawler_name}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def start_crawler(self, crawler_name):\n",
    "        \"\"\"Iniciar el crawler\"\"\"\n",
    "        try:\n",
    "            # Verificar si el crawler ya está en ejecución\n",
    "            crawler_info = self.client_glue.get_crawler(Name=crawler_name)\n",
    "            crawler_state = crawler_info.get('Crawler', {}).get('State')\n",
    "            \n",
    "            if crawler_state == 'RUNNING':\n",
    "                self.logger.info(f\"El crawler {crawler_name} ya está en ejecución\")\n",
    "                return True\n",
    "                \n",
    "            self.client_glue.start_crawler(\n",
    "                Name=crawler_name\n",
    "            )\n",
    "            self.logger.info(f\"Crawler {crawler_name} iniciado exitosamente\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al iniciar crawler {crawler_name}: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def update_dynamodb_attribute(self, table_name, key_name, key_value, attr_name, attr_value):\n",
    "        \"\"\"Actualizar atributo en DynamoDB\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f'Actualizando DynamoDB: {key_name}={key_value}, {attr_name}={attr_value}, tabla={table_name}')\n",
    "            dynamo_table = self.dynamodb.Table(table_name)\n",
    "            response = dynamo_table.update_item(\n",
    "                Key={key_name: key_value},\n",
    "                AttributeUpdates={\n",
    "                    attr_name: {\n",
    "                        'Value': attr_value,\n",
    "                        'Action': 'PUT'\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al actualizar DynamoDB: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def batch_update_dynamodb_attributes(self, table_name, items_to_update):\n",
    "        \"\"\"Actualizar múltiples atributos en DynamoDB usando batch write\"\"\"\n",
    "        try:\n",
    "            if not items_to_update:\n",
    "                return True\n",
    "                \n",
    "            self.logger.info(f'Actualizando {len(items_to_update)} items en DynamoDB con batch write')\n",
    "            \n",
    "            # Implementar lógica de batch_write_item\n",
    "            # Nota: batch_write_item tiene límite de 25 items por llamada\n",
    "            batch_size = 25\n",
    "            for i in range(0, len(items_to_update), batch_size):\n",
    "                batch = items_to_update[i:i+batch_size]\n",
    "                request_items = {\n",
    "                    table_name: [\n",
    "                        {\n",
    "                            'PutRequest': {\n",
    "                                'Item': item\n",
    "                            }\n",
    "                        } for item in batch\n",
    "                    ]\n",
    "                }\n",
    "                self.dynamodb.batch_write_item(RequestItems=request_items)\n",
    "                \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error en batch update DynamoDB: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def get_dynamodb_item(self, table_name, key_name, key_value):\n",
    "        \"\"\"Obtener un item específico de DynamoDB\"\"\"\n",
    "        try:\n",
    "            dynamo_table = self.dynamodb.Table(table_name)\n",
    "            response = dynamo_table.get_item(Key={key_name: key_value})\n",
    "            return response.get('Item')\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al obtener item de DynamoDB: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_dynamodb_items(self, table_name):\n",
    "        \"\"\"Obtener todos los items de una tabla DynamoDB\"\"\"\n",
    "        try:\n",
    "            dynamo_table = self.dynamodb.Table(table_name)\n",
    "            response = dynamo_table.scan()\n",
    "            items = response['Items']\n",
    "            \n",
    "            # Manejar paginación si hay más de 1MB de datos\n",
    "            while 'LastEvaluatedKey' in response:\n",
    "                response = dynamo_table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n",
    "                items.extend(response['Items'])\n",
    "                \n",
    "            return items\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error al obtener items de DynamoDB: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "# Funciones del flujo de trabajo personalizado\n",
    "def get_tables_and_prepare_delta_targets(glue_crawler_manager, config_table, endpoint_name, s3_target, team, data_source):\n",
    "    \"\"\"Obtener las tablas y preparar targets en una sola función optimizada\"\"\"\n",
    "    delta_targets = []\n",
    "    empty_tables = []\n",
    "    total_list = []\n",
    "    tables_to_update = []\n",
    "    \n",
    "    try:\n",
    "        # Una sola llamada a DynamoDB para obtener todas las tablas\n",
    "        all_items = glue_crawler_manager.get_dynamodb_items(config_table)\n",
    "        \n",
    "        # Filtrar localmente por endpoint\n",
    "        endpoint_items = [item for item in all_items if item.get('ENDPOINT') == endpoint_name]\n",
    "        \n",
    "        for item in endpoint_items:\n",
    "            table_name = item.get('TARGET_TABLE_NAME')\n",
    "            if not table_name:\n",
    "                continue\n",
    "                \n",
    "            total_list.append(table_name)\n",
    "            \n",
    "            # Verificar si la tabla necesita ser añadida al crawler\n",
    "            if 'CRAWLER' not in item or not item['CRAWLER']:\n",
    "                empty_tables.append(table_name)\n",
    "                tables_to_update.append(table_name)\n",
    "            \n",
    "            # Preparar delta target para esta tabla\n",
    "            if 'STAGE_TABLE_NAME' in item:\n",
    "                data_source_target = {\n",
    "                    'DeltaTables': [f\"{s3_target}{team}/{data_source}/{item['ENDPOINT']}/{item['STAGE_TABLE_NAME']}/\"],\n",
    "                    'ConnectionName': '',\n",
    "                    'CreateNativeDeltaTable': True\n",
    "                }\n",
    "                delta_targets.append(data_source_target)\n",
    "        \n",
    "        # Actualizar todas las tablas vacías en una sola operación por lotes (si es posible)\n",
    "        if tables_to_update:\n",
    "            # Aquí podríamos implementar una operación por lotes si usamos batch_write_item\n",
    "            for table in tables_to_update:\n",
    "                glue_crawler_manager.update_dynamodb_attribute(\n",
    "                    config_table, 'TARGET_TABLE_NAME', table, 'CRAWLER', True\n",
    "                )\n",
    "                logger.info(f\"Tabla {table} añadida al crawler\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al procesar tablas para el endpoint {endpoint_name}: {str(e)}\")\n",
    "    \n",
    "    return total_list, empty_tables, delta_targets\n",
    "\n",
    "def main():\n",
    "    \"\"\"Función principal\"\"\"\n",
    "    try:        \n",
    "        # Inicializar el gestor de Glue\n",
    "        glue_crawler_manager = GlueCrawlerManager(logger)\n",
    "        \n",
    "        # Variables principales\n",
    "        dynamo_config_table = args['DYNAMO_CONFIG_TABLE']\n",
    "        dynamo_endpoint_table = args['DYNAMO_ENDPOINT_TABLE']\n",
    "        s3_target = args['S3_STAGE_PREFIX']\n",
    "        arn_role_crawler = args['ARN_ROLE_CRAWLER']\n",
    "        job_name = args['JOB_NAME']\n",
    "        endpoint_name = args['ENDPOINT']\n",
    "        team = args['TEAM']\n",
    "        data_source = args['DATA_SOURCE']\n",
    "        process_id = args['PROCESS_ID']\n",
    "        \n",
    "        # Obtener datos del endpoint (una sola llamada a DynamoDB)\n",
    "        endpoint_data = glue_crawler_manager.get_dynamodb_item(dynamo_endpoint_table, 'ENDPOINT_NAME', endpoint_name)\n",
    "\n",
    "        if not endpoint_data:\n",
    "            logger.error(f\"No se encontraron datos para el endpoint {endpoint_name}\")\n",
    "            return\n",
    "        \n",
    "        # Definir nombres de recursos\n",
    "        data_catalog_database_name = f\"{team}_{data_source}_{endpoint_name}_stage\".lower()\n",
    "        data_catalog_crawler_name = data_catalog_database_name + \"_crawler\"\n",
    "        \n",
    "        logger.info(f\"Procesando endpoint: {endpoint_name}\")\n",
    "        logger.info(f\"Base de datos: {data_catalog_database_name}\")\n",
    "        logger.info(f\"Crawler: {data_catalog_crawler_name}\")\n",
    "        \n",
    "        # Obtener tablas y preparar targets en una sola operación (optimizado)\n",
    "        total_list, empty_tables, delta_targets = get_tables_and_prepare_delta_targets(\n",
    "            glue_crawler_manager, dynamo_config_table, endpoint_name, \n",
    "            s3_target, team, data_source\n",
    "        )\n",
    "        \n",
    "        if not total_list:\n",
    "            logger.warning(f\"No se encontraron tablas para el endpoint {endpoint_name}\")\n",
    "            return\n",
    "            \n",
    "        logger.info(f\"Total de tablas: {len(total_list)}, Tablas sin crawler: {len(empty_tables)}\")\n",
    "        \n",
    "        if not delta_targets:\n",
    "            logger.error(\"No se pudieron preparar objetivos Delta para el crawler\")\n",
    "            return\n",
    "        \n",
    "        # Comprobar si existe el crawler\n",
    "        if glue_crawler_manager.crawler_exists(data_catalog_crawler_name):\n",
    "            if empty_table:\n",
    "                logger.info(f\"Actualizando crawler con {len(empty_table)} nuevas tablas\")\n",
    "                glue_crawler_manager.update_delta_crawler(\n",
    "                    data_catalog_crawler_name, arn_role_crawler, \n",
    "                    data_catalog_database_name, delta_targets\n",
    "                )\n",
    "            \n",
    "            logger.info(f\"Iniciando crawler existente {data_catalog_crawler_name}\")\n",
    "            glue_crawler_manager.start_crawler(data_catalog_crawler_name)\n",
    "        else:\n",
    "            logger.info(\"Verificando existencia de la base de datos en el catálogo\")\n",
    "            \n",
    "            if glue_crawler_manager.database_exists(data_catalog_database_name):\n",
    "                logger.info(f\"Creando nuevo crawler para base de datos existente {data_catalog_database_name}\")\n",
    "                glue_crawler_manager.create_delta_crawler(\n",
    "                    data_catalog_crawler_name, arn_role_crawler, \n",
    "                    data_catalog_database_name, delta_targets\n",
    "                )\n",
    "                glue_crawler_manager.start_crawler(data_catalog_crawler_name)\n",
    "            else:\n",
    "                logger.info(\"Obteniendo el ARN del rol del trabajo\")\n",
    "                logger.info(f\"Glue Job: {job_name}\")\n",
    "                job_role_arn = glue_crawler_manager.get_job_role_arn(job_name)\n",
    "                \n",
    "                if not job_role_arn:\n",
    "                    logger.error(f\"No se pudo obtener el ARN del rol para el trabajo {job_name}\")\n",
    "                    return\n",
    "                \n",
    "                logger.info(\"Asignando permisos de etiquetas LF\")\n",
    "                glue_crawler_manager.grant_lf_tag_permissions(job_role_arn)\n",
    "                \n",
    "                logger.info(f\"Creando base de datos {data_catalog_database_name}\")\n",
    "                if not glue_crawler_manager.create_database(data_catalog_database_name):\n",
    "                    logger.error(\"No se pudo crear la base de datos. Abortando.\")\n",
    "                    return\n",
    "                \n",
    "                logger.info(\"Añadiendo etiquetas LF a la base de datos\")\n",
    "                glue_crawler_manager.add_lf_tags_to_database(data_catalog_database_name)\n",
    "                \n",
    "                logger.info(\"Asignando permisos a la base de datos\")\n",
    "                glue_crawler_manager.grant_database_permissions(job_role_arn, data_catalog_database_name)\n",
    "                \n",
    "                logger.info(\"Creando nuevo crawler\")\n",
    "                if glue_crawler_manager.create_delta_crawler(\n",
    "                    data_catalog_crawler_name, arn_role_crawler, \n",
    "                    data_catalog_database_name, delta_targets\n",
    "                ):\n",
    "                    logger.info(\"Iniciando nuevo crawler\")\n",
    "                    glue_crawler_manager.start_crawler(data_catalog_crawler_name)\n",
    "                else:\n",
    "                    logger.error(\"No se pudo crear el crawler. Abortando.\")\n",
    "                    \n",
    "        logger.info(\"Proceso completado exitosamente\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error en la función principal: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52938001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
